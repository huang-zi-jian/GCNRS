I0423 09:13:28.129616 23748 trainer.py:118] Test: [{'precision': 0.018776371308016862, 'recall': 0.10425897356277103, 'hit_ratio': 0.29535864978902954, 'ndcg': 0.051814157209944124}]
I0423 09:13:28.749566 23748 trainer.py:136] Epoch[0/200] loss: 0.6708199441432953
I0423 09:13:29.426856 23748 trainer.py:136] Epoch[1/200] loss: 0.627888164917628
I0423 09:13:30.065719 23748 trainer.py:136] Epoch[2/200] loss: 0.5978030383586883
I0423 09:13:30.704582 23748 trainer.py:136] Epoch[3/200] loss: 0.5611751000086467
I0423 09:13:31.357964 23748 trainer.py:136] Epoch[4/200] loss: 0.5360569566488266
I0423 09:13:31.997444 23748 trainer.py:136] Epoch[5/200] loss: 0.5046292255322139
I0423 09:13:32.623000 23748 trainer.py:136] Epoch[6/200] loss: 0.46542614698410034
I0423 09:13:33.337166 23748 trainer.py:136] Epoch[7/200] loss: 0.434887969493866
I0423 09:13:34.143706 23748 trainer.py:136] Epoch[8/200] loss: 0.4227688372135162
I0423 09:13:34.778380 23748 trainer.py:136] Epoch[9/200] loss: 0.38706959088643395
I0423 09:13:35.441156 23748 trainer.py:136] Epoch[10/200] loss: 0.38347067733605705
I0423 09:13:36.141386 23748 trainer.py:136] Epoch[11/200] loss: 0.3410404125849406
I0423 09:13:36.814686 23748 trainer.py:136] Epoch[12/200] loss: 0.3490118225415548
I0423 09:13:37.462520 23748 trainer.py:136] Epoch[13/200] loss: 0.32483535061279933
I0423 09:13:38.169813 23748 trainer.py:136] Epoch[14/200] loss: 0.3217032035191854
I0423 09:13:39.165031 23748 trainer.py:136] Epoch[15/200] loss: 0.30578361252943675
I0423 09:13:40.185214 23748 trainer.py:136] Epoch[16/200] loss: 0.3153102566798528
I0423 09:13:41.185429 23748 trainer.py:136] Epoch[17/200] loss: 0.3000371779004733
I0423 09:13:42.195447 23748 trainer.py:136] Epoch[18/200] loss: 0.31679327189922335
I0423 09:13:43.243865 23748 trainer.py:136] Epoch[19/200] loss: 0.29566463778416313
I0423 09:13:44.323904 23748 trainer.py:136] Epoch[20/200] loss: 0.28225224167108537
I0423 09:13:45.378972 23748 trainer.py:136] Epoch[21/200] loss: 0.29159829020500183
I0423 09:13:46.400142 23748 trainer.py:136] Epoch[22/200] loss: 0.2683319360017776
I0423 09:13:47.439222 23748 trainer.py:136] Epoch[23/200] loss: 0.2721098199486732
I0423 09:13:48.432478 23748 trainer.py:136] Epoch[24/200] loss: 0.2567347084482511
I0423 09:13:49.419719 23748 trainer.py:136] Epoch[25/200] loss: 0.2968144843975703
I0423 09:13:50.426966 23748 trainer.py:136] Epoch[26/200] loss: 0.25863542407751083
I0423 09:13:51.455121 23748 trainer.py:136] Epoch[27/200] loss: 0.2533649797240893
I0423 09:13:52.467280 23748 trainer.py:136] Epoch[28/200] loss: 0.2637879808743795
I0423 09:13:53.457529 23748 trainer.py:136] Epoch[29/200] loss: 0.26966569423675535
I0423 09:13:54.447818 23748 trainer.py:136] Epoch[30/200] loss: 0.2589009031653404
I0423 09:13:55.430089 23748 trainer.py:136] Epoch[31/200] loss: 0.25748448769251503
I0423 09:13:56.447302 23748 trainer.py:136] Epoch[32/200] loss: 0.2545892144242922
I0423 09:13:57.455149 23748 trainer.py:136] Epoch[33/200] loss: 0.25003519107898076
I0423 09:13:58.508127 23748 trainer.py:136] Epoch[34/200] loss: 0.2307636265953382
I0423 09:13:59.523822 23748 trainer.py:136] Epoch[35/200] loss: 0.2390568494796753
I0423 09:14:00.560951 23748 trainer.py:136] Epoch[36/200] loss: 0.23910686522722244
I0423 09:14:01.565938 23748 trainer.py:136] Epoch[37/200] loss: 0.24852914710839588
I0423 09:14:02.600304 23748 trainer.py:136] Epoch[38/200] loss: 0.24083381444215773
I0423 09:14:03.621106 23748 trainer.py:136] Epoch[39/200] loss: 0.24436724931001663
I0423 09:14:04.655195 23748 trainer.py:136] Epoch[40/200] loss: 0.23604904115200043
I0423 09:14:05.709250 23748 trainer.py:136] Epoch[41/200] loss: 0.22045670102039974
I0423 09:14:06.696580 23748 trainer.py:136] Epoch[42/200] loss: 0.2343785251180331
I0423 09:14:07.669964 23748 trainer.py:136] Epoch[43/200] loss: 0.2352645829319954
I0423 09:14:08.683247 23748 trainer.py:136] Epoch[44/200] loss: 0.2176440477371216
I0423 09:14:09.706412 23748 trainer.py:136] Epoch[45/200] loss: 0.22405983408292135
I0423 09:14:10.733544 23748 trainer.py:136] Epoch[46/200] loss: 0.23554491698741914
I0423 09:14:11.712880 23748 trainer.py:136] Epoch[47/200] loss: 0.22153642078240712
I0423 09:14:12.724042 23748 trainer.py:136] Epoch[48/200] loss: 0.22302188724279404
I0423 09:14:13.720157 23748 trainer.py:136] Epoch[49/200] loss: 0.20774011611938475
I0423 09:14:13.741086 23748 trainer.py:142] Test: [{'precision': 0.02383966244725736, 'recall': 0.15963191754330988, 'hit_ratio': 0.3881856540084388, 'ndcg': 0.07456032483191302}]
I0423 09:14:14.756256 23748 trainer.py:136] Epoch[50/200] loss: 0.21094310333331426
I0423 09:14:15.802133 23748 trainer.py:136] Epoch[51/200] loss: 0.2073798432946205
I0423 09:14:16.806316 23748 trainer.py:136] Epoch[52/200] loss: 0.20558480223019918
I0423 09:14:17.836108 23748 trainer.py:136] Epoch[53/200] loss: 0.21182847569386165
I0423 09:14:18.817048 23748 trainer.py:136] Epoch[54/200] loss: 0.20241356740395228
I0423 09:14:19.841274 23748 trainer.py:136] Epoch[55/200] loss: 0.18943655490875244
I0423 09:14:20.857466 23748 trainer.py:136] Epoch[56/200] loss: 0.22375316321849822
I0423 09:14:21.858712 23748 trainer.py:136] Epoch[57/200] loss: 0.21130728696783382
I0423 09:14:22.858918 23748 trainer.py:136] Epoch[58/200] loss: 0.21152839908997217
I0423 09:14:23.914178 23748 trainer.py:136] Epoch[59/200] loss: 0.20169650440414746
I0423 09:14:24.934308 23748 trainer.py:136] Epoch[60/200] loss: 0.20012709548075994
I0423 09:14:25.986930 23748 trainer.py:136] Epoch[61/200] loss: 0.20954566473762196
I0423 09:14:27.006131 23748 trainer.py:136] Epoch[62/200] loss: 0.20353322128454845
I0423 09:14:28.053216 23748 trainer.py:136] Epoch[63/200] loss: 0.19748848925034204
I0423 09:14:29.066431 23748 trainer.py:136] Epoch[64/200] loss: 0.19547746032476426
I0423 09:14:30.101511 23748 trainer.py:136] Epoch[65/200] loss: 0.19992666939894357
I0423 09:14:31.096878 23748 trainer.py:136] Epoch[66/200] loss: 0.19350364257891972
I0423 09:14:32.109040 23748 trainer.py:136] Epoch[67/200] loss: 0.19763238752881687
I0423 09:14:33.159422 23748 trainer.py:136] Epoch[68/200] loss: 0.1983185316125552
I0423 09:14:34.202505 23748 trainer.py:136] Epoch[69/200] loss: 0.19783647110064825
I0423 09:14:35.245630 23748 trainer.py:136] Epoch[70/200] loss: 0.1922602708141009
I0423 09:14:36.276785 23748 trainer.py:136] Epoch[71/200] loss: 0.18309477890531223
I0423 09:14:37.287946 23748 trainer.py:136] Epoch[72/200] loss: 0.1850355530778567
I0423 09:14:38.287163 23748 trainer.py:136] Epoch[73/200] loss: 0.19059500644604366
I0423 09:14:39.286394 23748 trainer.py:136] Epoch[74/200] loss: 0.19160097887118657
I0423 09:14:40.301556 23748 trainer.py:136] Epoch[75/200] loss: 0.19151655882596968
I0423 09:14:41.288804 23748 trainer.py:136] Epoch[76/200] loss: 0.18901261140902836
I0423 09:14:42.294046 23748 trainer.py:136] Epoch[77/200] loss: 0.19761688262224197
I0423 09:14:43.295281 23748 trainer.py:136] Epoch[78/200] loss: 0.19199925363063813
I0423 09:14:44.276576 23748 trainer.py:136] Epoch[79/200] loss: 0.18194550921519598
I0423 09:14:45.256887 23748 trainer.py:136] Epoch[80/200] loss: 0.17943613628546398
I0423 09:14:46.240161 23748 trainer.py:136] Epoch[81/200] loss: 0.19483473499615986
I0423 09:14:47.239357 23748 trainer.py:136] Epoch[82/200] loss: 0.183650903403759
I0423 09:14:48.231648 23748 trainer.py:136] Epoch[83/200] loss: 0.1794614260395368
I0423 09:14:49.227883 23748 trainer.py:136] Epoch[84/200] loss: 0.1756508727868398
I0423 09:14:50.213170 23748 trainer.py:136] Epoch[85/200] loss: 0.16347135975956917
I0423 09:14:51.205420 23748 trainer.py:136] Epoch[86/200] loss: 0.18787521968285242
I0423 09:14:52.231523 23748 trainer.py:136] Epoch[87/200] loss: 0.17739816283186277
I0423 09:14:53.246680 23748 trainer.py:136] Epoch[88/200] loss: 0.1693453495701154
I0423 09:14:54.236921 23748 trainer.py:136] Epoch[89/200] loss: 0.18217563033103942
I0423 09:14:55.236181 23748 trainer.py:136] Epoch[90/200] loss: 0.1856709693868955
I0423 09:14:56.260257 23748 trainer.py:136] Epoch[91/200] loss: 0.18431784957647324
I0423 09:14:57.280412 23748 trainer.py:136] Epoch[92/200] loss: 0.18239784091711045
I0423 09:14:58.289136 23748 trainer.py:136] Epoch[93/200] loss: 0.1856855218609174
I0423 09:14:59.310286 23748 trainer.py:136] Epoch[94/200] loss: 0.1772133914132913
I0423 09:15:00.301530 23748 trainer.py:136] Epoch[95/200] loss: 0.17381625125805536
I0423 09:15:01.299763 23748 trainer.py:136] Epoch[96/200] loss: 0.17658811037739117
I0423 09:15:02.321927 23748 trainer.py:136] Epoch[97/200] loss: 0.16915834099054336
I0423 09:15:03.322231 23748 trainer.py:136] Epoch[98/200] loss: 0.17536294137438138
I0423 09:15:04.329436 23748 trainer.py:136] Epoch[99/200] loss: 0.1631769155462583
I0423 09:15:04.350365 23748 trainer.py:142] Test: [{'precision': 0.023417721518987317, 'recall': 0.16283356821331504, 'hit_ratio': 0.38396624472573837, 'ndcg': 0.07820692499491894}]
I0423 09:15:05.359556 23748 trainer.py:136] Epoch[100/200] loss: 0.17787017971277236
I0423 09:15:06.396672 23748 trainer.py:136] Epoch[101/200] loss: 0.17854482432206473
I0423 09:15:07.429755 23748 trainer.py:136] Epoch[102/200] loss: 0.17795317818721135
I0423 09:15:08.430970 23748 trainer.py:136] Epoch[103/200] loss: 0.17149704396724702
I0423 09:15:09.413263 23748 trainer.py:136] Epoch[104/200] loss: 0.17071809296806653
I0423 09:15:10.418499 23748 trainer.py:136] Epoch[105/200] loss: 0.1719919020930926
I0423 09:15:11.414756 23748 trainer.py:136] Epoch[106/200] loss: 0.17390154699484509
I0423 09:15:12.423985 23748 trainer.py:136] Epoch[107/200] loss: 0.16036933561166128
I0423 09:15:13.454099 23748 trainer.py:136] Epoch[108/200] loss: 0.16477666199207305
I0423 09:15:14.486245 23748 trainer.py:136] Epoch[109/200] loss: 0.1688985322912534
I0423 09:15:15.487493 23748 trainer.py:136] Epoch[110/200] loss: 0.15916646445790927
I0423 09:15:16.515566 23748 trainer.py:136] Epoch[111/200] loss: 0.16319355343778927
I0423 09:15:17.557654 23748 trainer.py:136] Epoch[112/200] loss: 0.17016281535228092
I0423 09:15:18.559870 23748 trainer.py:136] Epoch[113/200] loss: 0.15511020968357722
I0423 09:15:19.586111 23748 trainer.py:136] Epoch[114/200] loss: 0.1630302632848422
I0423 09:15:20.592305 23748 trainer.py:136] Epoch[115/200] loss: 0.17298082063595455
I0423 09:15:21.602460 23748 trainer.py:136] Epoch[116/200] loss: 0.1619167481859525
I0423 09:15:22.607705 23748 trainer.py:136] Epoch[117/200] loss: 0.17261136720577877
I0423 09:15:23.609315 23748 trainer.py:136] Epoch[118/200] loss: 0.15823428283135096
I0423 09:15:24.591652 23748 trainer.py:136] Epoch[119/200] loss: 0.16946217616399128
I0423 09:15:25.578517 23748 trainer.py:136] Epoch[120/200] loss: 0.16696457266807557
I0423 09:15:26.553871 23748 trainer.py:136] Epoch[121/200] loss: 0.15577242871125538
I0423 09:15:27.539024 23748 trainer.py:136] Epoch[122/200] loss: 0.16387329796950023
I0423 09:15:28.541243 23748 trainer.py:136] Epoch[123/200] loss: 0.16540435031056405
I0423 09:15:29.550459 23748 trainer.py:136] Epoch[124/200] loss: 0.16013693089286488
I0423 09:15:30.565591 23748 trainer.py:136] Epoch[125/200] loss: 0.16480659892161686
I0423 09:15:31.564645 23748 trainer.py:136] Epoch[126/200] loss: 0.17298254494865736
I0423 09:15:32.599519 23748 trainer.py:136] Epoch[127/200] loss: 0.16114485089977582
I0423 09:15:33.630644 23748 trainer.py:136] Epoch[128/200] loss: 0.1681908292074998
I0423 09:15:34.622458 23748 trainer.py:136] Epoch[129/200] loss: 0.17483136455217999
I0423 09:15:35.640686 23748 trainer.py:136] Epoch[130/200] loss: 0.16437689463297525
I0423 09:15:36.647904 23748 trainer.py:136] Epoch[131/200] loss: 0.165492690851291
I0423 09:15:37.662066 23748 trainer.py:136] Epoch[132/200] loss: 0.16158170774579048
I0423 09:15:38.679827 23748 trainer.py:136] Epoch[133/200] loss: 0.15863367517789204
I0423 09:15:39.688038 23748 trainer.py:136] Epoch[134/200] loss: 0.1588029757142067
I0423 09:15:40.707187 23748 trainer.py:136] Epoch[135/200] loss: 0.15748909239967665
I0423 09:15:41.715403 23748 trainer.py:136] Epoch[136/200] loss: 0.17365825921297073
I0423 09:15:42.729580 23748 trainer.py:136] Epoch[137/200] loss: 0.1675931453704834
I0423 09:15:43.720828 23748 trainer.py:136] Epoch[138/200] loss: 0.16924003958702089
I0423 09:15:44.736012 23748 trainer.py:136] Epoch[139/200] loss: 0.1639487030605475
I0423 09:15:45.739211 23748 trainer.py:136] Epoch[140/200] loss: 0.16768025234341621
I0423 09:15:46.736423 23748 trainer.py:136] Epoch[141/200] loss: 0.16747399990757306
I0423 09:15:47.769473 23748 trainer.py:136] Epoch[142/200] loss: 0.16448935295144718
I0423 09:15:48.743280 23748 trainer.py:136] Epoch[143/200] loss: 0.1654822513461113
I0423 09:15:49.733604 23748 trainer.py:136] Epoch[144/200] loss: 0.15580734809239705
I0423 09:15:50.742776 23748 trainer.py:136] Epoch[145/200] loss: 0.15748624106248219
I0423 09:15:51.744976 23748 trainer.py:136] Epoch[146/200] loss: 0.17242341240247092
I0423 09:15:52.744201 23748 trainer.py:136] Epoch[147/200] loss: 0.15503224606315294
I0423 09:15:53.761413 23748 trainer.py:136] Epoch[148/200] loss: 0.1683468076090018
I0423 09:15:54.781561 23748 trainer.py:136] Epoch[149/200] loss: 0.16336502159635227
I0423 09:15:54.808101 23748 trainer.py:142] Test: [{'precision': 0.022784810126582258, 'recall': 0.15266177797823363, 'hit_ratio': 0.3628691983122363, 'ndcg': 0.07607401914678803}]
I0423 09:15:55.800397 23748 trainer.py:136] Epoch[150/200] loss: 0.15950714101394017
I0423 09:15:56.809542 23748 trainer.py:136] Epoch[151/200] loss: 0.16546038885911304
I0423 09:15:57.820703 23748 trainer.py:136] Epoch[152/200] loss: 0.1779647114376227
I0423 09:15:58.829887 23748 trainer.py:136] Epoch[153/200] loss: 0.160072510689497
I0423 09:15:59.837080 23748 trainer.py:136] Epoch[154/200] loss: 0.16655694221456846
I0423 09:16:00.836332 23748 trainer.py:136] Epoch[155/200] loss: 0.16461633816361426
I0423 09:16:01.837611 23748 trainer.py:136] Epoch[156/200] loss: 0.16303671474258105
I0423 09:16:02.803963 23748 trainer.py:136] Epoch[157/200] loss: 0.14770198067029316
I0423 09:16:03.805152 23748 trainer.py:136] Epoch[158/200] loss: 0.15839718108375866
I0423 09:16:04.790418 23748 trainer.py:136] Epoch[159/200] loss: 0.14652929107348125
I0423 09:16:05.785653 23748 trainer.py:136] Epoch[160/200] loss: 0.1623447060585022
I0423 09:16:06.781325 23748 trainer.py:136] Epoch[161/200] loss: 0.15591952055692673
I0423 09:16:07.773621 23748 trainer.py:136] Epoch[162/200] loss: 0.15509282449881237
I0423 09:16:08.785856 23748 trainer.py:136] Epoch[163/200] loss: 0.1515677221119404
I0423 09:16:09.805094 23748 trainer.py:136] Epoch[164/200] loss: 0.16136311416824659
I0423 09:16:10.817325 23748 trainer.py:136] Epoch[165/200] loss: 0.15637033606568973
I0423 09:16:11.840516 23748 trainer.py:136] Epoch[166/200] loss: 0.1607821429769198
I0423 09:16:12.836757 23748 trainer.py:136] Epoch[167/200] loss: 0.15698205903172494
I0423 09:16:13.835897 23748 trainer.py:136] Epoch[168/200] loss: 0.16120068008701008
I0423 09:16:14.820260 23748 trainer.py:136] Epoch[169/200] loss: 0.16257954637209573
I0423 09:16:15.826503 23748 trainer.py:136] Epoch[170/200] loss: 0.1545621007680893
I0423 09:16:16.822725 23748 trainer.py:136] Epoch[171/200] loss: 0.1515047791103522
I0423 09:16:17.827907 23748 trainer.py:136] Epoch[172/200] loss: 0.15765550260742506
I0423 09:16:18.809174 23748 trainer.py:136] Epoch[173/200] loss: 0.16729870090881985
I0423 09:16:19.808445 23748 trainer.py:136] Epoch[174/200] loss: 0.15295293554663658
I0423 09:16:20.813681 23748 trainer.py:136] Epoch[175/200] loss: 0.14980103050669033
I0423 09:16:21.791018 23748 trainer.py:136] Epoch[176/200] loss: 0.15768060187498728
I0423 09:16:22.796229 23748 trainer.py:136] Epoch[177/200] loss: 0.15413788259029387
I0423 09:16:23.789907 23748 trainer.py:136] Epoch[178/200] loss: 0.15836240748564404
I0423 09:16:24.779597 23748 trainer.py:136] Epoch[179/200] loss: 0.1674179365237554
I0423 09:16:25.771485 23748 trainer.py:136] Epoch[180/200] loss: 0.1512344906727473
I0423 09:16:26.749614 23748 trainer.py:136] Epoch[181/200] loss: 0.16165604591369628
I0423 09:16:27.725957 23748 trainer.py:136] Epoch[182/200] loss: 0.16154137030243873
I0423 09:16:28.702240 23748 trainer.py:136] Epoch[183/200] loss: 0.16344298397501308
I0423 09:16:29.697462 23748 trainer.py:136] Epoch[184/200] loss: 0.1552845577398936
I0423 09:16:30.713602 23748 trainer.py:136] Epoch[185/200] loss: 0.1577044429878394
I0423 09:16:31.430747 23748 trainer.py:136] Epoch[186/200] loss: 0.15810196995735168
I0423 09:16:32.109571 23748 trainer.py:136] Epoch[187/200] loss: 0.15600244378050168
I0423 09:16:32.768367 23748 trainer.py:136] Epoch[188/200] loss: 0.16098048835992812
I0423 09:16:33.426727 23748 trainer.py:136] Epoch[189/200] loss: 0.14941253811120986
I0423 09:16:34.080130 23748 trainer.py:136] Epoch[190/200] loss: 0.15421032160520554
I0423 09:16:34.798281 23748 trainer.py:136] Epoch[191/200] loss: 0.14915703857938448
I0423 09:16:35.459071 23748 trainer.py:136] Epoch[192/200] loss: 0.16206114292144774
I0423 09:16:36.105475 23748 trainer.py:136] Epoch[193/200] loss: 0.15497990051905314
I0423 09:16:36.729424 23748 trainer.py:136] Epoch[194/200] loss: 0.15346662054459254
I0423 09:16:37.364814 23748 trainer.py:136] Epoch[195/200] loss: 0.15203180958827336
I0423 09:16:37.982677 23748 trainer.py:136] Epoch[196/200] loss: 0.16090234915415447
I0423 09:16:38.602936 23748 trainer.py:136] Epoch[197/200] loss: 0.1583726264536381
I0423 09:16:39.231477 23748 trainer.py:136] Epoch[198/200] loss: 0.15711683730284373
I0423 09:16:39.856996 23748 trainer.py:136] Epoch[199/200] loss: 0.15021870906154314
I0423 09:16:39.869954 23748 trainer.py:142] Test: [{'precision': 0.022995780590717278, 'recall': 0.16310983905920612, 'hit_ratio': 0.37130801687763715, 'ndcg': 0.07998190754722471}]
