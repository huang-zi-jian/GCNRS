I0423 21:08:39.947301 31536 trainer.py:118] Test: [{'precision': 0.015822784810126576, 'recall': 0.09773645248328791, 'hit_ratio': 0.26582278481012656, 'ndcg': 0.05478870807318634}]
I0423 21:08:40.262248 31536 trainer.py:136] Epoch[0/200] loss: 0.6932120005289714
I0423 21:08:40.570265 31536 trainer.py:136] Epoch[1/200] loss: 0.6464576204617818
I0423 21:08:40.885211 31536 trainer.py:136] Epoch[2/200] loss: 0.6163870473702748
I0423 21:08:41.186204 31536 trainer.py:136] Epoch[3/200] loss: 0.5749363124370575
I0423 21:08:41.492181 31536 trainer.py:136] Epoch[4/200] loss: 0.5312341411908468
I0423 21:08:41.800689 31536 trainer.py:136] Epoch[5/200] loss: 0.5021698236465454
I0423 21:08:42.175359 31536 trainer.py:136] Epoch[6/200] loss: 0.45505788226922356
I0423 21:08:42.528179 31536 trainer.py:136] Epoch[7/200] loss: 0.4193725695212682
I0423 21:08:42.892071 31536 trainer.py:136] Epoch[8/200] loss: 0.4206580231587092
I0423 21:08:43.263828 31536 trainer.py:136] Epoch[9/200] loss: 0.373087606827418
I0423 21:08:43.657166 31536 trainer.py:136] Epoch[10/200] loss: 0.38617999653021495
I0423 21:08:43.995036 31536 trainer.py:136] Epoch[11/200] loss: 0.3437707672516505
I0423 21:08:44.327922 31536 trainer.py:136] Epoch[12/200] loss: 0.3370909869670868
I0423 21:08:44.665119 31536 trainer.py:136] Epoch[13/200] loss: 0.33305381834506986
I0423 21:08:44.996012 31536 trainer.py:136] Epoch[14/200] loss: 0.3256682818134626
I0423 21:08:45.346838 31536 trainer.py:136] Epoch[15/200] loss: 0.3245834564169248
I0423 21:08:45.691299 31536 trainer.py:136] Epoch[16/200] loss: 0.3209043726325035
I0423 21:08:46.031741 31536 trainer.py:136] Epoch[17/200] loss: 0.304455707470576
I0423 21:08:46.355657 31536 trainer.py:136] Epoch[18/200] loss: 0.32414697408676146
I0423 21:08:46.695095 31536 trainer.py:136] Epoch[19/200] loss: 0.2905086353421211
I0423 21:08:47.036951 31536 trainer.py:136] Epoch[20/200] loss: 0.3081592222054799
I0423 21:08:47.424655 31536 trainer.py:136] Epoch[21/200] loss: 0.28063211739063265
I0423 21:08:47.857850 31536 trainer.py:136] Epoch[22/200] loss: 0.2813795725504557
I0423 21:08:48.266358 31536 trainer.py:136] Epoch[23/200] loss: 0.27236263304948805
I0423 21:08:48.662617 31536 trainer.py:136] Epoch[24/200] loss: 0.2626668393611908
I0423 21:08:49.054306 31536 trainer.py:136] Epoch[25/200] loss: 0.28078925162553786
I0423 21:08:49.430050 31536 trainer.py:136] Epoch[26/200] loss: 0.2646429821848869
I0423 21:08:49.819334 31536 trainer.py:136] Epoch[27/200] loss: 0.26000114977359773
I0423 21:08:50.196074 31536 trainer.py:136] Epoch[28/200] loss: 0.26721165676911673
I0423 21:08:50.563443 31536 trainer.py:136] Epoch[29/200] loss: 0.2616092965006828
I0423 21:08:50.949153 31536 trainer.py:136] Epoch[30/200] loss: 0.25025749057531355
I0423 21:08:51.327886 31536 trainer.py:136] Epoch[31/200] loss: 0.2670053685704867
I0423 21:08:51.700244 31536 trainer.py:136] Epoch[32/200] loss: 0.2604659780859947
I0423 21:08:52.046081 31536 trainer.py:136] Epoch[33/200] loss: 0.26236433486143745
I0423 21:08:52.389931 31536 trainer.py:136] Epoch[34/200] loss: 0.26032254497210183
I0423 21:08:52.825083 31536 trainer.py:136] Epoch[35/200] loss: 0.23237401495377222
I0423 21:08:53.248667 31536 trainer.py:136] Epoch[36/200] loss: 0.24344692081212999
I0423 21:08:53.670521 31536 trainer.py:136] Epoch[37/200] loss: 0.2303341085712115
I0423 21:08:54.048258 31536 trainer.py:136] Epoch[38/200] loss: 0.22877990305423737
I0423 21:08:54.463867 31536 trainer.py:136] Epoch[39/200] loss: 0.2354469781120618
I0423 21:08:54.836194 31536 trainer.py:136] Epoch[40/200] loss: 0.23365449011325837
I0423 21:08:55.231871 31536 trainer.py:136] Epoch[41/200] loss: 0.23328497310479482
I0423 21:08:55.602196 31536 trainer.py:136] Epoch[42/200] loss: 0.23917297522226968
I0423 21:08:55.977939 31536 trainer.py:136] Epoch[43/200] loss: 0.2154822751879692
I0423 21:08:56.336739 31536 trainer.py:136] Epoch[44/200] loss: 0.22711245814959208
I0423 21:08:56.692734 31536 trainer.py:136] Epoch[45/200] loss: 0.2215107758839925
I0423 21:08:57.078443 31536 trainer.py:136] Epoch[46/200] loss: 0.22159790967901546
I0423 21:08:57.507009 31536 trainer.py:136] Epoch[47/200] loss: 0.2199516455332438
I0423 21:08:57.933138 31536 trainer.py:136] Epoch[48/200] loss: 0.23132428576548894
I0423 21:08:58.421504 31536 trainer.py:136] Epoch[49/200] loss: 0.2291842723886172
I0423 21:08:58.443430 31536 trainer.py:142] Test: [{'precision': 0.022573839662447245, 'recall': 0.1527653312463439, 'hit_ratio': 0.3628691983122363, 'ndcg': 0.07610814888591463}]
I0423 21:08:58.872585 31536 trainer.py:136] Epoch[50/200] loss: 0.2117822801073392
I0423 21:08:59.260289 31536 trainer.py:136] Epoch[51/200] loss: 0.2022776628533999
I0423 21:08:59.642562 31536 trainer.py:136] Epoch[52/200] loss: 0.2098454490303993
I0423 21:09:00.045214 31536 trainer.py:136] Epoch[53/200] loss: 0.2144203007221222
I0423 21:09:00.483747 31536 trainer.py:136] Epoch[54/200] loss: 0.21221454590559005
I0423 21:09:00.846116 31536 trainer.py:136] Epoch[55/200] loss: 0.214912644525369
I0423 21:09:01.197939 31536 trainer.py:136] Epoch[56/200] loss: 0.19923622235655786
I0423 21:09:01.573246 31536 trainer.py:136] Epoch[57/200] loss: 0.202289413412412
I0423 21:09:01.943008 31536 trainer.py:136] Epoch[58/200] loss: 0.19833668420712153
I0423 21:09:02.327721 31536 trainer.py:136] Epoch[59/200] loss: 0.199565988779068
I0423 21:09:02.701041 31536 trainer.py:136] Epoch[60/200] loss: 0.19885078618923824
I0423 21:09:03.062831 31536 trainer.py:136] Epoch[61/200] loss: 0.2010854942103227
I0423 21:09:03.425617 31536 trainer.py:136] Epoch[62/200] loss: 0.19812033995985984
I0423 21:09:03.791028 31536 trainer.py:136] Epoch[63/200] loss: 0.19747999409834543
I0423 21:09:04.155663 31536 trainer.py:136] Epoch[64/200] loss: 0.20178128331899642
I0423 21:09:04.510476 31536 trainer.py:136] Epoch[65/200] loss: 0.20565169403950373
I0423 21:09:04.865845 31536 trainer.py:136] Epoch[66/200] loss: 0.205214357872804
I0423 21:09:05.240591 31536 trainer.py:136] Epoch[67/200] loss: 0.2075396920243899
I0423 21:09:05.609916 31536 trainer.py:136] Epoch[68/200] loss: 0.1946181982755661
I0423 21:09:05.978683 31536 trainer.py:136] Epoch[69/200] loss: 0.19130565524101256
I0423 21:09:06.381730 31536 trainer.py:136] Epoch[70/200] loss: 0.20028857017556825
I0423 21:09:06.758042 31536 trainer.py:136] Epoch[71/200] loss: 0.19118747115135193
I0423 21:09:07.099899 31536 trainer.py:136] Epoch[72/200] loss: 0.18531610469023388
I0423 21:09:07.441756 31536 trainer.py:136] Epoch[73/200] loss: 0.19830422153075536
I0423 21:09:07.792158 31536 trainer.py:136] Epoch[74/200] loss: 0.18922117898861568
I0423 21:09:08.150957 31536 trainer.py:136] Epoch[75/200] loss: 0.19359333366155623
I0423 21:09:08.510754 31536 trainer.py:136] Epoch[76/200] loss: 0.17540891220172247
I0423 21:09:08.845268 31536 trainer.py:136] Epoch[77/200] loss: 0.18954335500796635
I0423 21:09:09.174823 31536 trainer.py:136] Epoch[78/200] loss: 0.18554897059996922
I0423 21:09:09.505716 31536 trainer.py:136] Epoch[79/200] loss: 0.18028726999958355
I0423 21:09:09.851193 31536 trainer.py:136] Epoch[80/200] loss: 0.18106326560179392
I0423 21:09:10.203016 31536 trainer.py:136] Epoch[81/200] loss: 0.17780473629633586
I0423 21:09:10.539888 31536 trainer.py:136] Epoch[82/200] loss: 0.189046186208725
I0423 21:09:10.869336 31536 trainer.py:136] Epoch[83/200] loss: 0.17564785728851953
I0423 21:09:11.191259 31536 trainer.py:136] Epoch[84/200] loss: 0.19111536939938864
I0423 21:09:11.515176 31536 trainer.py:136] Epoch[85/200] loss: 0.17749359061320621
I0423 21:09:11.875938 31536 trainer.py:136] Epoch[86/200] loss: 0.18281473343571028
I0423 21:09:12.216799 31536 trainer.py:136] Epoch[87/200] loss: 0.17715319593747456
I0423 21:09:12.553240 31536 trainer.py:136] Epoch[88/200] loss: 0.17485142921408017
I0423 21:09:12.886126 31536 trainer.py:136] Epoch[89/200] loss: 0.1726868599653244
I0423 21:09:13.216022 31536 trainer.py:136] Epoch[90/200] loss: 0.17369794473052025
I0423 21:09:13.577383 31536 trainer.py:136] Epoch[91/200] loss: 0.17731646895408631
I0423 21:09:13.941213 31536 trainer.py:136] Epoch[92/200] loss: 0.1663854864736398
I0423 21:09:14.291042 31536 trainer.py:136] Epoch[93/200] loss: 0.17608803982535998
I0423 21:09:14.651399 31536 trainer.py:136] Epoch[94/200] loss: 0.17596834500630695
I0423 21:09:15.017176 31536 trainer.py:136] Epoch[95/200] loss: 0.17275982846816382
I0423 21:09:15.405875 31536 trainer.py:136] Epoch[96/200] loss: 0.1766719420750936
I0423 21:09:15.774202 31536 trainer.py:136] Epoch[97/200] loss: 0.17644665191570919
I0423 21:09:16.178848 31536 trainer.py:136] Epoch[98/200] loss: 0.16877323041359585
I0423 21:09:16.596043 31536 trainer.py:136] Epoch[99/200] loss: 0.1709046130379041
I0423 21:09:16.610994 31536 trainer.py:142] Test: [{'precision': 0.023839662447257364, 'recall': 0.16169332308572815, 'hit_ratio': 0.38396624472573837, 'ndcg': 0.07741994211207559}]
I0423 21:09:17.020623 31536 trainer.py:136] Epoch[100/200] loss: 0.1789981171488762
I0423 21:09:17.552376 31536 trainer.py:136] Epoch[101/200] loss: 0.1578164386252562
I0423 21:09:18.172544 31536 trainer.py:136] Epoch[102/200] loss: 0.16744147861997286
I0423 21:09:18.792041 31536 trainer.py:136] Epoch[103/200] loss: 0.17003582740823428
I0423 21:09:19.423927 31536 trainer.py:136] Epoch[104/200] loss: 0.1707037903368473
I0423 21:09:20.063377 31536 trainer.py:136] Epoch[105/200] loss: 0.1642461011807124
I0423 21:09:20.742762 31536 trainer.py:136] Epoch[106/200] loss: 0.17754749630888303
I0423 21:09:21.378634 31536 trainer.py:136] Epoch[107/200] loss: 0.17218963106473287
I0423 21:09:22.020065 31536 trainer.py:136] Epoch[108/200] loss: 0.17801349957784016
I0423 21:09:22.653508 31536 trainer.py:136] Epoch[109/200] loss: 0.17276956215500833
I0423 21:09:23.260477 31536 trainer.py:136] Epoch[110/200] loss: 0.17843697716792425
I0423 21:09:23.886614 31536 trainer.py:136] Epoch[111/200] loss: 0.1613022652765115
I0423 21:09:24.527471 31536 trainer.py:136] Epoch[112/200] loss: 0.16784491166472434
I0423 21:09:25.163920 31536 trainer.py:136] Epoch[113/200] loss: 0.1715168702105681
I0423 21:09:25.781438 31536 trainer.py:136] Epoch[114/200] loss: 0.17543320258458456
I0423 21:09:26.421298 31536 trainer.py:136] Epoch[115/200] loss: 0.16114487648010253
I0423 21:09:27.101591 31536 trainer.py:136] Epoch[116/200] loss: 0.17990891312559446
I0423 21:09:27.774887 31536 trainer.py:136] Epoch[117/200] loss: 0.16793159743150074
I0423 21:09:28.490492 31536 trainer.py:136] Epoch[118/200] loss: 0.1618206632634004
I0423 21:09:29.191688 31536 trainer.py:136] Epoch[119/200] loss: 0.1752471663057804
I0423 21:09:30.139090 31536 trainer.py:136] Epoch[120/200] loss: 0.1534986545642217
I0423 21:09:31.141310 31536 trainer.py:136] Epoch[121/200] loss: 0.16316206182042758
I0423 21:09:32.137596 31536 trainer.py:136] Epoch[122/200] loss: 0.15725243389606475
I0423 21:09:33.165750 31536 trainer.py:136] Epoch[123/200] loss: 0.15566972667972248
I0423 21:09:34.196863 31536 trainer.py:136] Epoch[124/200] loss: 0.1573559023439884
I0423 21:09:35.201062 31536 trainer.py:136] Epoch[125/200] loss: 0.17110541289051373
I0423 21:09:36.190339 31536 trainer.py:136] Epoch[126/200] loss: 0.16652293205261232
I0423 21:09:37.213462 31536 trainer.py:136] Epoch[127/200] loss: 0.1587585213283698
I0423 21:09:38.223619 31536 trainer.py:136] Epoch[128/200] loss: 0.16998859519759815
I0423 21:09:39.234798 31536 trainer.py:136] Epoch[129/200] loss: 0.1651434580485026
I0423 21:09:40.254929 31536 trainer.py:136] Epoch[130/200] loss: 0.18344705551862717
I0423 21:09:41.333889 31536 trainer.py:136] Epoch[131/200] loss: 0.1676030012468497
I0423 21:09:42.574822 31536 trainer.py:136] Epoch[132/200] loss: 0.16742559398214021
I0423 21:09:43.934810 31536 trainer.py:136] Epoch[133/200] loss: 0.155456738670667
I0423 21:09:45.291863 31536 trainer.py:136] Epoch[134/200] loss: 0.17313883950312933
I0423 21:09:46.658459 31536 trainer.py:136] Epoch[135/200] loss: 0.16335407719016076
I0423 21:09:47.989571 31536 trainer.py:136] Epoch[136/200] loss: 0.16282604038715362
I0423 21:09:49.335630 31536 trainer.py:136] Epoch[137/200] loss: 0.16154480079809824
I0423 21:09:50.676265 31536 trainer.py:136] Epoch[138/200] loss: 0.15176919226845106
I0423 21:09:52.048235 31536 trainer.py:136] Epoch[139/200] loss: 0.16414439727862676
I0423 21:09:53.516926 31536 trainer.py:136] Epoch[140/200] loss: 0.174235034485658
I0423 21:09:55.263221 31536 trainer.py:136] Epoch[141/200] loss: 0.17129761775334676
I0423 21:09:56.960598 31536 trainer.py:136] Epoch[142/200] loss: 0.1660498504837354
I0423 21:09:58.649097 31536 trainer.py:136] Epoch[143/200] loss: 0.16272834514578183
I0423 21:10:00.342000 31536 trainer.py:136] Epoch[144/200] loss: 0.15697085733215013
I0423 21:10:02.060402 31536 trainer.py:136] Epoch[145/200] loss: 0.16328407203157744
I0423 21:10:03.755847 31536 trainer.py:136] Epoch[146/200] loss: 0.16597350736459096
I0423 21:10:05.446742 31536 trainer.py:136] Epoch[147/200] loss: 0.1669897360106309
I0423 21:10:07.151232 31536 trainer.py:136] Epoch[148/200] loss: 0.17081011285384495
I0423 21:10:08.867618 31536 trainer.py:136] Epoch[149/200] loss: 0.16337309976418812
I0423 21:10:08.896522 31536 trainer.py:142] Test: [{'precision': 0.02489451476793246, 'recall': 0.16629951155267608, 'hit_ratio': 0.3924050632911392, 'ndcg': 0.0791994888792582}]
I0423 21:10:10.608931 31536 trainer.py:136] Epoch[150/200] loss: 0.15737603480617204
I0423 21:10:12.292909 31536 trainer.py:136] Epoch[151/200] loss: 0.15996746147672336
I0423 21:10:13.976416 31536 trainer.py:136] Epoch[152/200] loss: 0.16144852911432583
I0423 21:10:15.672887 31536 trainer.py:136] Epoch[153/200] loss: 0.15733827302853268
I0423 21:10:17.343868 31536 trainer.py:136] Epoch[154/200] loss: 0.16012979174653688
I0423 21:10:19.024417 31536 trainer.py:136] Epoch[155/200] loss: 0.1543775163590908
I0423 21:10:20.709975 31536 trainer.py:136] Epoch[156/200] loss: 0.16391264249881107
I0423 21:10:22.414585 31536 trainer.py:136] Epoch[157/200] loss: 0.15120664114753404
I0423 21:10:24.104597 31536 trainer.py:136] Epoch[158/200] loss: 0.15946016286810238
I0423 21:10:25.801100 31536 trainer.py:136] Epoch[159/200] loss: 0.1634742873410384
I0423 21:10:27.480111 31536 trainer.py:136] Epoch[160/200] loss: 0.15920191879073778
I0423 21:10:29.157613 31536 trainer.py:136] Epoch[161/200] loss: 0.16139347702264786
I0423 21:10:30.840109 31536 trainer.py:136] Epoch[162/200] loss: 0.16116674070556958
I0423 21:10:32.520627 31536 trainer.py:136] Epoch[163/200] loss: 0.15589863260587056
I0423 21:10:34.204165 31536 trainer.py:136] Epoch[164/200] loss: 0.1572559873263041
I0423 21:10:35.882739 31536 trainer.py:136] Epoch[165/200] loss: 0.16264197180668513
I0423 21:10:37.548263 31536 trainer.py:136] Epoch[166/200] loss: 0.15704351166884103
I0423 21:10:39.231616 31536 trainer.py:136] Epoch[167/200] loss: 0.1609998181462288
I0423 21:10:40.918123 31536 trainer.py:136] Epoch[168/200] loss: 0.16343226209282874
I0423 21:10:42.599646 31536 trainer.py:136] Epoch[169/200] loss: 0.15031181747714678
I0423 21:10:44.276175 31536 trainer.py:136] Epoch[170/200] loss: 0.15557794471581776
I0423 21:10:45.955064 31536 trainer.py:136] Epoch[171/200] loss: 0.15834568540255228
I0423 21:10:47.632979 31536 trainer.py:136] Epoch[172/200] loss: 0.14804710894823075
I0423 21:10:49.307977 31536 trainer.py:136] Epoch[173/200] loss: 0.16045073519150416
I0423 21:10:50.984922 31536 trainer.py:136] Epoch[174/200] loss: 0.15136876131097476
I0423 21:10:52.644153 31536 trainer.py:136] Epoch[175/200] loss: 0.1577777606745561
I0423 21:10:54.309191 31536 trainer.py:136] Epoch[176/200] loss: 0.15857181300719578
I0423 21:10:55.980748 31536 trainer.py:136] Epoch[177/200] loss: 0.1587408423423767
I0423 21:10:57.652964 31536 trainer.py:136] Epoch[178/200] loss: 0.15543901125590007
I0423 21:10:59.320926 31536 trainer.py:136] Epoch[179/200] loss: 0.15530709500114123
I0423 21:11:01.000427 31536 trainer.py:136] Epoch[180/200] loss: 0.165730037043492
I0423 21:11:02.673939 31536 trainer.py:136] Epoch[181/200] loss: 0.1575901950399081
I0423 21:11:04.342954 31536 trainer.py:136] Epoch[182/200] loss: 0.1559928928812345
I0423 21:11:06.026473 31536 trainer.py:136] Epoch[183/200] loss: 0.1562305229405562
I0423 21:11:07.689123 31536 trainer.py:136] Epoch[184/200] loss: 0.15273667499423027
I0423 21:11:09.360110 31536 trainer.py:136] Epoch[185/200] loss: 0.14592622816562653
I0423 21:11:11.035655 31536 trainer.py:136] Epoch[186/200] loss: 0.14833541040619214
I0423 21:11:12.703209 31536 trainer.py:136] Epoch[187/200] loss: 0.16321430280804633
I0423 21:11:14.378191 31536 trainer.py:136] Epoch[188/200] loss: 0.1572293515006701
I0423 21:11:16.068575 31536 trainer.py:136] Epoch[189/200] loss: 0.15535048618912697
I0423 21:11:17.762182 31536 trainer.py:136] Epoch[190/200] loss: 0.16333536182840666
I0423 21:11:19.434906 31536 trainer.py:136] Epoch[191/200] loss: 0.1638698307176431
I0423 21:11:21.121281 31536 trainer.py:136] Epoch[192/200] loss: 0.16015619660417238
I0423 21:11:22.795148 31536 trainer.py:136] Epoch[193/200] loss: 0.15352292284369468
I0423 21:11:24.480321 31536 trainer.py:136] Epoch[194/200] loss: 0.15239979152878125
I0423 21:11:26.151902 31536 trainer.py:136] Epoch[195/200] loss: 0.16684538175662358
I0423 21:11:27.830490 31536 trainer.py:136] Epoch[196/200] loss: 0.16690057044227918
I0423 21:11:29.482514 31536 trainer.py:136] Epoch[197/200] loss: 0.15825831020871797
I0423 21:11:31.152395 31536 trainer.py:136] Epoch[198/200] loss: 0.15996677652001381
I0423 21:11:32.808880 31536 trainer.py:136] Epoch[199/200] loss: 0.15042690907915432
I0423 21:11:32.837094 31536 trainer.py:142] Test: [{'precision': 0.023839662447257364, 'recall': 0.1775743759288063, 'hit_ratio': 0.4155274261603376, 'ndcg': 0.08200780239916004}]
