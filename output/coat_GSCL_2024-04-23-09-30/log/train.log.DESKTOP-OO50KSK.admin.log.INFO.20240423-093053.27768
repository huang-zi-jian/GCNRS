I0423 09:30:55.059872 34772 trainer.py:118] Test: [{'precision': 0.014556962025316453, 'recall': 0.07337356736090912, 'hit_ratio': 0.21940928270042195, 'ndcg': 0.037040804879877556}]
I0423 09:30:56.825054 34772 trainer.py:136] Epoch[0/200] loss: 0.6761768023173015
I0423 09:30:58.622588 34772 trainer.py:136] Epoch[1/200] loss: 0.643487932284673
I0423 09:31:00.304552 34772 trainer.py:136] Epoch[2/200] loss: 0.6059873461723327
I0423 09:31:01.988440 34772 trainer.py:136] Epoch[3/200] loss: 0.560941086212794
I0423 09:31:03.658433 34772 trainer.py:136] Epoch[4/200] loss: 0.5328124850988388
I0423 09:31:05.363452 34772 trainer.py:136] Epoch[5/200] loss: 0.488686924179395
I0423 09:31:07.043074 34772 trainer.py:136] Epoch[6/200] loss: 0.4632468064626058
I0423 09:31:08.716088 34772 trainer.py:136] Epoch[7/200] loss: 0.41942412753899894
I0423 09:31:10.414526 34772 trainer.py:136] Epoch[8/200] loss: 0.4251201142867406
I0423 09:31:12.100031 34772 trainer.py:136] Epoch[9/200] loss: 0.37457033395767214
I0423 09:31:13.774022 34772 trainer.py:136] Epoch[10/200] loss: 0.370352765917778
I0423 09:31:15.477463 34772 trainer.py:136] Epoch[11/200] loss: 0.35372915466626487
I0423 09:31:17.166193 34772 trainer.py:136] Epoch[12/200] loss: 0.3490363736947378
I0423 09:31:18.846693 34772 trainer.py:136] Epoch[13/200] loss: 0.3314589262008667
I0423 09:31:20.517672 34772 trainer.py:136] Epoch[14/200] loss: 0.33095466246207556
I0423 09:31:22.187646 34772 trainer.py:136] Epoch[15/200] loss: 0.3147209157546361
I0423 09:31:23.908017 34772 trainer.py:136] Epoch[16/200] loss: 0.3152915174762408
I0423 09:31:25.592959 34772 trainer.py:136] Epoch[17/200] loss: 0.304245496292909
I0423 09:31:27.264520 34772 trainer.py:136] Epoch[18/200] loss: 0.3119943171739578
I0423 09:31:29.045665 34772 trainer.py:136] Epoch[19/200] loss: 0.2888846009969711
I0423 09:31:30.796972 34772 trainer.py:136] Epoch[20/200] loss: 0.2892400473356247
I0423 09:31:32.578600 34772 trainer.py:136] Epoch[21/200] loss: 0.2803275376558304
I0423 09:31:34.262583 34772 trainer.py:136] Epoch[22/200] loss: 0.30516925901174546
I0423 09:31:35.943183 34772 trainer.py:136] Epoch[23/200] loss: 0.2968518336613973
I0423 09:31:37.650094 34772 trainer.py:136] Epoch[24/200] loss: 0.2731348102291425
I0423 09:31:39.336638 34772 trainer.py:136] Epoch[25/200] loss: 0.2938808153072993
I0423 09:31:41.018144 34772 trainer.py:136] Epoch[26/200] loss: 0.2602601796388626
I0423 09:31:42.746160 34772 trainer.py:136] Epoch[27/200] loss: 0.27218040426572165
I0423 09:31:44.461602 34772 trainer.py:136] Epoch[28/200] loss: 0.2642691756288211
I0423 09:31:46.163091 34772 trainer.py:136] Epoch[29/200] loss: 0.2629153514901797
I0423 09:31:47.875532 34772 trainer.py:136] Epoch[30/200] loss: 0.25358252276976906
I0423 09:31:49.569665 34772 trainer.py:136] Epoch[31/200] loss: 0.2629826600352923
I0423 09:31:51.260079 34772 trainer.py:136] Epoch[32/200] loss: 0.24327203184366225
I0423 09:31:52.973772 34772 trainer.py:136] Epoch[33/200] loss: 0.24293512850999832
I0423 09:31:54.678053 34772 trainer.py:136] Epoch[34/200] loss: 0.25536569704612094
I0423 09:31:56.362547 34772 trainer.py:136] Epoch[35/200] loss: 0.23684085359176
I0423 09:31:58.051981 34772 trainer.py:136] Epoch[36/200] loss: 0.2530726874868075
I0423 09:31:59.757924 34772 trainer.py:136] Epoch[37/200] loss: 0.2250222201148669
I0423 09:32:01.537115 34772 trainer.py:136] Epoch[38/200] loss: 0.22607181171576182
I0423 09:32:03.313301 34772 trainer.py:136] Epoch[39/200] loss: 0.2284767041603724
I0423 09:32:05.126367 34772 trainer.py:136] Epoch[40/200] loss: 0.22285671879847843
I0423 09:32:06.842168 34772 trainer.py:136] Epoch[41/200] loss: 0.22586735387643178
I0423 09:32:08.535531 34772 trainer.py:136] Epoch[42/200] loss: 0.22602413942416508
I0423 09:32:10.229972 34772 trainer.py:136] Epoch[43/200] loss: 0.2347716917594274
I0423 09:32:11.954955 34772 trainer.py:136] Epoch[44/200] loss: 0.22063439736763638
I0423 09:32:13.649899 34772 trainer.py:136] Epoch[45/200] loss: 0.20987137854099275
I0423 09:32:15.383222 34772 trainer.py:136] Epoch[46/200] loss: 0.21864302506049474
I0423 09:32:17.059770 34772 trainer.py:136] Epoch[47/200] loss: 0.22529550592104594
I0423 09:32:18.771601 34772 trainer.py:136] Epoch[48/200] loss: 0.22088000575701397
I0423 09:32:20.452117 34772 trainer.py:136] Epoch[49/200] loss: 0.2282998815178871
I0423 09:32:20.481020 34772 trainer.py:142] Test: [{'precision': 0.0221518987341772, 'recall': 0.1476668783630809, 'hit_ratio': 0.3628691983122363, 'ndcg': 0.07435153156945763}]
I0423 09:32:22.156562 34772 trainer.py:136] Epoch[50/200] loss: 0.21844593038161594
I0423 09:32:23.826977 34772 trainer.py:136] Epoch[51/200] loss: 0.21092388778924942
I0423 09:32:25.520863 34772 trainer.py:136] Epoch[52/200] loss: 0.2083940992752711
I0423 09:32:27.218371 34772 trainer.py:136] Epoch[53/200] loss: 0.20359765042861303
I0423 09:32:28.919364 34772 trainer.py:136] Epoch[54/200] loss: 0.20806240737438203
I0423 09:32:30.705952 34772 trainer.py:136] Epoch[55/200] loss: 0.20856603533029555
I0423 09:32:32.496477 34772 trainer.py:136] Epoch[56/200] loss: 0.18202084029714266
I0423 09:32:34.219883 34772 trainer.py:136] Epoch[57/200] loss: 0.20593415002028148
I0423 09:32:35.911306 34772 trainer.py:136] Epoch[58/200] loss: 0.1918581247329712
I0423 09:32:37.587274 34772 trainer.py:136] Epoch[59/200] loss: 0.19939700216054917
I0423 09:32:39.288682 34772 trainer.py:136] Epoch[60/200] loss: 0.2062337706486384
I0423 09:32:40.960274 34772 trainer.py:136] Epoch[61/200] loss: 0.20233193784952164
I0423 09:32:42.632287 34772 trainer.py:136] Epoch[62/200] loss: 0.19513271699349086
I0423 09:32:44.354654 34772 trainer.py:136] Epoch[63/200] loss: 0.20018692165613175
I0423 09:32:46.054130 34772 trainer.py:136] Epoch[64/200] loss: 0.20376569628715516
I0423 09:32:47.738070 34772 trainer.py:136] Epoch[65/200] loss: 0.1933982029557228
I0423 09:32:49.454700 34772 trainer.py:136] Epoch[66/200] loss: 0.20434241890907287
I0423 09:32:51.129242 34772 trainer.py:136] Epoch[67/200] loss: 0.19596691677967706
I0423 09:32:52.807777 34772 trainer.py:136] Epoch[68/200] loss: 0.20499143799146016
I0423 09:32:54.470765 34772 trainer.py:136] Epoch[69/200] loss: 0.21047957688570024
I0423 09:32:56.184665 34772 trainer.py:136] Epoch[70/200] loss: 0.200887085745732
I0423 09:32:57.880102 34772 trainer.py:136] Epoch[71/200] loss: 0.19281357973814012
I0423 09:32:59.582018 34772 trainer.py:136] Epoch[72/200] loss: 0.19668805301189424
I0423 09:33:01.370157 34772 trainer.py:136] Epoch[73/200] loss: 0.188220772643884
I0423 09:33:03.141413 34772 trainer.py:136] Epoch[74/200] loss: 0.19121995866298674
I0423 09:33:04.834894 34772 trainer.py:136] Epoch[75/200] loss: 0.18356099079052607
I0423 09:33:06.520856 34772 trainer.py:136] Epoch[76/200] loss: 0.19726921965678532
I0423 09:33:08.198968 34772 trainer.py:136] Epoch[77/200] loss: 0.186836335559686
I0423 09:33:09.888445 34772 trainer.py:136] Epoch[78/200] loss: 0.18123930965860685
I0423 09:33:11.577349 34772 trainer.py:136] Epoch[79/200] loss: 0.18158054451147715
I0423 09:33:13.299266 34772 trainer.py:136] Epoch[80/200] loss: 0.18219373722871143
I0423 09:33:14.977390 34772 trainer.py:136] Epoch[81/200] loss: 0.17333717346191407
I0423 09:33:16.652880 34772 trainer.py:136] Epoch[82/200] loss: 0.18657083262999852
I0423 09:33:18.374283 34772 trainer.py:136] Epoch[83/200] loss: 0.1871642564733823
I0423 09:33:20.047780 34772 trainer.py:136] Epoch[84/200] loss: 0.1807130570213
I0423 09:33:21.729058 34772 trainer.py:136] Epoch[85/200] loss: 0.1935025396446387
I0423 09:33:23.413525 34772 trainer.py:136] Epoch[86/200] loss: 0.18322568833827974
I0423 09:33:25.124636 34772 trainer.py:136] Epoch[87/200] loss: 0.18840841750303905
I0423 09:33:26.806131 34772 trainer.py:136] Epoch[88/200] loss: 0.17815780540307363
I0423 09:33:28.484740 34772 trainer.py:136] Epoch[89/200] loss: 0.16998913834492366
I0423 09:33:30.161275 34772 trainer.py:136] Epoch[90/200] loss: 0.17264422277609506
I0423 09:33:31.845335 34772 trainer.py:136] Epoch[91/200] loss: 0.1661376083890597
I0423 09:33:33.577467 34772 trainer.py:136] Epoch[92/200] loss: 0.17213534985979398
I0423 09:33:35.263830 34772 trainer.py:136] Epoch[93/200] loss: 0.19140436400969824
I0423 09:33:37.035007 34772 trainer.py:136] Epoch[94/200] loss: 0.16747463196516038
I0423 09:33:38.754799 34772 trainer.py:136] Epoch[95/200] loss: 0.17322935809691747
I0423 09:33:40.438571 34772 trainer.py:136] Epoch[96/200] loss: 0.17367184037963548
I0423 09:33:42.120107 34772 trainer.py:136] Epoch[97/200] loss: 0.16838298067450524
I0423 09:33:43.800075 34772 trainer.py:136] Epoch[98/200] loss: 0.19569827144344648
I0423 09:33:45.469040 34772 trainer.py:136] Epoch[99/200] loss: 0.17190402025977772
I0423 09:33:45.503923 34772 trainer.py:142] Test: [{'precision': 0.02341772151898733, 'recall': 0.15479968929336016, 'hit_ratio': 0.3755274261603376, 'ndcg': 0.07960498172542771}]
I0423 09:33:47.179086 34772 trainer.py:136] Epoch[100/200] loss: 0.18096688513954481
I0423 09:33:48.848647 34772 trainer.py:136] Epoch[101/200] loss: 0.17138691917061805
I0423 09:33:50.523589 34772 trainer.py:136] Epoch[102/200] loss: 0.172336013118426
I0423 09:33:52.199132 34772 trainer.py:136] Epoch[103/200] loss: 0.17466624702016512
I0423 09:33:53.873711 34772 trainer.py:136] Epoch[104/200] loss: 0.1689367117981116
I0423 09:33:55.543727 34772 trainer.py:136] Epoch[105/200] loss: 0.17796122084061305
I0423 09:33:57.230225 34772 trainer.py:136] Epoch[106/200] loss: 0.1638494516412417
I0423 09:33:58.907759 34772 trainer.py:136] Epoch[107/200] loss: 0.16727528323729832
I0423 09:34:00.581748 34772 trainer.py:136] Epoch[108/200] loss: 0.1631473811964194
I0423 09:34:02.250092 34772 trainer.py:136] Epoch[109/200] loss: 0.17051783949136734
I0423 09:34:03.946586 34772 trainer.py:136] Epoch[110/200] loss: 0.15858256071805954
I0423 09:34:05.630797 34772 trainer.py:136] Epoch[111/200] loss: 0.1650336171189944
I0423 09:34:07.312340 34772 trainer.py:136] Epoch[112/200] loss: 0.17088016470273334
I0423 09:34:09.003885 34772 trainer.py:136] Epoch[113/200] loss: 0.16784827932715415
I0423 09:34:10.666927 34772 trainer.py:136] Epoch[114/200] loss: 0.16807354539632796
I0423 09:34:12.341650 34772 trainer.py:136] Epoch[115/200] loss: 0.16632974967360498
I0423 09:34:14.007294 34772 trainer.py:136] Epoch[116/200] loss: 0.1665100286404292
I0423 09:34:15.676268 34772 trainer.py:136] Epoch[117/200] loss: 0.17090468630194663
I0423 09:34:17.359750 34772 trainer.py:136] Epoch[118/200] loss: 0.16685179397463798
I0423 09:34:19.029328 34772 trainer.py:136] Epoch[119/200] loss: 0.1693701259791851
I0423 09:34:20.719237 34772 trainer.py:136] Epoch[120/200] loss: 0.1621442159016927
I0423 09:34:22.391801 34772 trainer.py:136] Epoch[121/200] loss: 0.16807075167695681
I0423 09:34:24.065875 34772 trainer.py:136] Epoch[122/200] loss: 0.1633084995051225
I0423 09:34:25.759436 34772 trainer.py:136] Epoch[123/200] loss: 0.15956044445435205
I0423 09:34:27.441852 34772 trainer.py:136] Epoch[124/200] loss: 0.16694605921705563
I0423 09:34:29.124424 34772 trainer.py:136] Epoch[125/200] loss: 0.15969046130776404
I0423 09:34:30.807643 34772 trainer.py:136] Epoch[126/200] loss: 0.17260303397973378
I0423 09:34:32.486178 34772 trainer.py:136] Epoch[127/200] loss: 0.17354218165079752
I0423 09:34:34.194653 34772 trainer.py:136] Epoch[128/200] loss: 0.16732690955201784
I0423 09:34:35.940979 34772 trainer.py:136] Epoch[129/200] loss: 0.1621603677670161
I0423 09:34:37.616088 34772 trainer.py:136] Epoch[130/200] loss: 0.16637878865003586
I0423 09:34:39.288695 34772 trainer.py:136] Epoch[131/200] loss: 0.1658601942161719
I0423 09:34:40.954248 34772 trainer.py:136] Epoch[132/200] loss: 0.1667272686958313
I0423 09:34:42.628201 34772 trainer.py:136] Epoch[133/200] loss: 0.16321786617239317
I0423 09:34:44.361562 34772 trainer.py:136] Epoch[134/200] loss: 0.17218734125296276
I0423 09:34:46.064094 34772 trainer.py:136] Epoch[135/200] loss: 0.16456675305962562
I0423 09:34:47.766411 34772 trainer.py:136] Epoch[136/200] loss: 0.16665352483590443
I0423 09:34:49.441489 34772 trainer.py:136] Epoch[137/200] loss: 0.1647147238254547
I0423 09:34:51.106836 34772 trainer.py:136] Epoch[138/200] loss: 0.15908688108126323
I0423 09:34:52.844235 34772 trainer.py:136] Epoch[139/200] loss: 0.1739554283519586
I0423 09:34:54.524494 34772 trainer.py:136] Epoch[140/200] loss: 0.1540761684377988
I0423 09:34:56.196134 34772 trainer.py:136] Epoch[141/200] loss: 0.16632680520415305
I0423 09:34:57.884957 34772 trainer.py:136] Epoch[142/200] loss: 0.1584287298222383
I0423 09:34:59.556673 34772 trainer.py:136] Epoch[143/200] loss: 0.15236110240221024
I0423 09:35:01.244662 34772 trainer.py:136] Epoch[144/200] loss: 0.16121814573804538
I0423 09:35:02.921468 34772 trainer.py:136] Epoch[145/200] loss: 0.1593865968286991
I0423 09:35:04.604101 34772 trainer.py:136] Epoch[146/200] loss: 0.1557109996676445
I0423 09:35:06.274232 34772 trainer.py:136] Epoch[147/200] loss: 0.16134834041198096
I0423 09:35:07.946742 34772 trainer.py:136] Epoch[148/200] loss: 0.16738107204437255
I0423 09:35:09.606766 34772 trainer.py:136] Epoch[149/200] loss: 0.15576841086149215
I0423 09:35:09.636666 34772 trainer.py:142] Test: [{'precision': 0.023839662447257364, 'recall': 0.16911024090770923, 'hit_ratio': 0.38396624472573837, 'ndcg': 0.08138770962428783}]
I0423 09:35:11.330153 34772 trainer.py:136] Epoch[150/200] loss: 0.162832510471344
I0423 09:35:13.005890 34772 trainer.py:136] Epoch[151/200] loss: 0.1535066120326519
I0423 09:35:14.683066 34772 trainer.py:136] Epoch[152/200] loss: 0.15520724902550379
I0423 09:35:16.346343 34772 trainer.py:136] Epoch[153/200] loss: 0.16775432353218397
I0423 09:35:18.022787 34772 trainer.py:136] Epoch[154/200] loss: 0.1591479706267516
I0423 09:35:19.699791 34772 trainer.py:136] Epoch[155/200] loss: 0.15867764552434285
I0423 09:35:21.376367 34772 trainer.py:136] Epoch[156/200] loss: 0.15365147665143014
I0423 09:35:23.053453 34772 trainer.py:136] Epoch[157/200] loss: 0.16141222044825554
I0423 09:35:24.728380 34772 trainer.py:136] Epoch[158/200] loss: 0.15097636977831522
I0423 09:35:26.397943 34772 trainer.py:136] Epoch[159/200] loss: 0.16407392745216687
I0423 09:35:28.082557 34772 trainer.py:136] Epoch[160/200] loss: 0.16635916953285534
I0423 09:35:29.776470 34772 trainer.py:136] Epoch[161/200] loss: 0.1660528083642324
I0423 09:35:31.436140 34772 trainer.py:136] Epoch[162/200] loss: 0.15400065258145332
I0423 09:35:33.102726 34772 trainer.py:136] Epoch[163/200] loss: 0.16790498221913974
I0423 09:35:34.776680 34772 trainer.py:136] Epoch[164/200] loss: 0.15375055347879726
I0423 09:35:36.447250 34772 trainer.py:136] Epoch[165/200] loss: 0.15751934722065924
I0423 09:35:38.126922 34772 trainer.py:136] Epoch[166/200] loss: 0.17124220629533132
I0423 09:35:39.826421 34772 trainer.py:136] Epoch[167/200] loss: 0.16352046007911364
I0423 09:35:41.598077 34772 trainer.py:136] Epoch[168/200] loss: 0.1553314263621966
I0423 09:35:43.262750 34772 trainer.py:136] Epoch[169/200] loss: 0.16030335103472074
I0423 09:35:44.937313 34772 trainer.py:136] Epoch[170/200] loss: 0.16786198591192564
I0423 09:35:46.602875 34772 trainer.py:136] Epoch[171/200] loss: 0.15637134313583373
I0423 09:35:48.293325 34772 trainer.py:136] Epoch[172/200] loss: 0.15843305165568988
I0423 09:35:49.976811 34772 trainer.py:136] Epoch[173/200] loss: 0.16787578016519547
I0423 09:35:51.660286 34772 trainer.py:136] Epoch[174/200] loss: 0.1574095423022906
I0423 09:35:53.344642 34772 trainer.py:136] Epoch[175/200] loss: 0.159354201455911
I0423 09:35:55.030243 34772 trainer.py:136] Epoch[176/200] loss: 0.15713974957664809
I0423 09:35:56.707207 34772 trainer.py:136] Epoch[177/200] loss: 0.1566295864681403
I0423 09:35:58.130357 34772 trainer.py:136] Epoch[178/200] loss: 0.16221122468511265
I0423 09:35:59.503236 34772 trainer.py:136] Epoch[179/200] loss: 0.1590604414542516
I0423 09:36:00.606145 34772 trainer.py:136] Epoch[180/200] loss: 0.16131781140963236
I0423 09:36:01.638984 34772 trainer.py:136] Epoch[181/200] loss: 0.16276431009173392
I0423 09:36:02.646376 34772 trainer.py:136] Epoch[182/200] loss: 0.15665011778473853
I0423 09:36:03.640964 34772 trainer.py:136] Epoch[183/200] loss: 0.15497757444779078
I0423 09:36:04.643207 34772 trainer.py:136] Epoch[184/200] loss: 0.1611952044069767
I0423 09:36:05.651364 34772 trainer.py:136] Epoch[185/200] loss: 0.15929593692223232
I0423 09:36:06.658958 34772 trainer.py:136] Epoch[186/200] loss: 0.16348221575220426
I0423 09:36:07.665416 34772 trainer.py:136] Epoch[187/200] loss: 0.14863833884398142
I0423 09:36:08.652666 34772 trainer.py:136] Epoch[188/200] loss: 0.15035246163606644
I0423 09:36:09.637942 34772 trainer.py:136] Epoch[189/200] loss: 0.14867190221945445
I0423 09:36:10.638147 34772 trainer.py:136] Epoch[190/200] loss: 0.15233498190840086
I0423 09:36:11.635699 34772 trainer.py:136] Epoch[191/200] loss: 0.15803348769744238
I0423 09:36:12.624970 34772 trainer.py:136] Epoch[192/200] loss: 0.17111924290657043
I0423 09:36:13.359080 34772 trainer.py:136] Epoch[193/200] loss: 0.15582547038793565
I0423 09:36:13.983555 34772 trainer.py:136] Epoch[194/200] loss: 0.1538587803641955
I0423 09:36:14.668265 34772 trainer.py:136] Epoch[195/200] loss: 0.1529407247900963
I0423 09:36:15.315679 34772 trainer.py:136] Epoch[196/200] loss: 0.15599505255619686
I0423 09:36:15.850469 34772 trainer.py:136] Epoch[197/200] loss: 0.16084245418508847
I0423 09:36:16.175382 34772 trainer.py:136] Epoch[198/200] loss: 0.15692256515224776
I0423 09:36:16.494314 34772 trainer.py:136] Epoch[199/200] loss: 0.15628002161780993
I0423 09:36:16.508268 34772 trainer.py:142] Test: [{'precision': 0.02299578059071728, 'recall': 0.1789758225834175, 'hit_ratio': 0.41396624472573837, 'ndcg': 0.08216230388778983}]
