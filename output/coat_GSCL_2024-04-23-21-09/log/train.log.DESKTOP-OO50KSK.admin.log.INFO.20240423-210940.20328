I0423 21:09:42.044056  2120 trainer.py:118] Test: [{'precision': 0.013924050632911392, 'recall': 0.07653696514456006, 'hit_ratio': 0.22362869198312235, 'ndcg': 0.0421406008325137}]
I0423 21:09:43.395078  2120 trainer.py:136] Epoch[0/200] loss: 0.6889784057935079
I0423 21:09:44.755656  2120 trainer.py:136] Epoch[1/200] loss: 0.6455339411894481
I0423 21:09:46.115708  2120 trainer.py:136] Epoch[2/200] loss: 0.6121438662211101
I0423 21:09:47.465759  2120 trainer.py:136] Epoch[3/200] loss: 0.5745341022809346
I0423 21:09:48.827330  2120 trainer.py:136] Epoch[4/200] loss: 0.524522586663564
I0423 21:09:50.163408  2120 trainer.py:136] Epoch[5/200] loss: 0.49219166735808056
I0423 21:09:51.529413  2120 trainer.py:136] Epoch[6/200] loss: 0.45783399442831674
I0423 21:09:52.974740  2120 trainer.py:136] Epoch[7/200] loss: 0.4348198503255844
I0423 21:09:54.618377  2120 trainer.py:136] Epoch[8/200] loss: 0.4176888177792231
I0423 21:09:56.326214  2120 trainer.py:136] Epoch[9/200] loss: 0.4107174426317215
I0423 21:09:58.020613  2120 trainer.py:136] Epoch[10/200] loss: 0.37373235325018567
I0423 21:09:59.707124  2120 trainer.py:136] Epoch[11/200] loss: 0.355487792690595
I0423 21:10:01.433918  2120 trainer.py:136] Epoch[12/200] loss: 0.32950021674235663
I0423 21:10:03.113430  2120 trainer.py:136] Epoch[13/200] loss: 0.3282273148496946
I0423 21:10:04.789939  2120 trainer.py:136] Epoch[14/200] loss: 0.336479348440965
I0423 21:10:06.473862  2120 trainer.py:136] Epoch[15/200] loss: 0.3183792655666669
I0423 21:10:08.168391  2120 trainer.py:136] Epoch[16/200] loss: 0.3139254634579023
I0423 21:10:09.898721  2120 trainer.py:136] Epoch[17/200] loss: 0.31886763870716095
I0423 21:10:11.600225  2120 trainer.py:136] Epoch[18/200] loss: 0.3100709646940231
I0423 21:10:13.300109  2120 trainer.py:136] Epoch[19/200] loss: 0.30721728801727294
I0423 21:10:15.004541  2120 trainer.py:136] Epoch[20/200] loss: 0.29625916530688606
I0423 21:10:16.714972  2120 trainer.py:136] Epoch[21/200] loss: 0.3025773053367933
I0423 21:10:18.417849  2120 trainer.py:136] Epoch[22/200] loss: 0.2795100932319959
I0423 21:10:20.103386  2120 trainer.py:136] Epoch[23/200] loss: 0.27578884214162824
I0423 21:10:21.774725  2120 trainer.py:136] Epoch[24/200] loss: 0.2769582708676656
I0423 21:10:23.441207  2120 trainer.py:136] Epoch[25/200] loss: 0.27411238302787144
I0423 21:10:25.106858  2120 trainer.py:136] Epoch[26/200] loss: 0.273567687968413
I0423 21:10:26.780451  2120 trainer.py:136] Epoch[27/200] loss: 0.28798488080501555
I0423 21:10:28.451419  2120 trainer.py:136] Epoch[28/200] loss: 0.2682867412765821
I0423 21:10:30.147866  2120 trainer.py:136] Epoch[29/200] loss: 0.26400763243436814
I0423 21:10:31.841898  2120 trainer.py:136] Epoch[30/200] loss: 0.2643862704435984
I0423 21:10:33.521841  2120 trainer.py:136] Epoch[31/200] loss: 0.2665707012017568
I0423 21:10:35.209445  2120 trainer.py:136] Epoch[32/200] loss: 0.2666212096810341
I0423 21:10:36.896910  2120 trainer.py:136] Epoch[33/200] loss: 0.23720999111731847
I0423 21:10:38.582335  2120 trainer.py:136] Epoch[34/200] loss: 0.255225067337354
I0423 21:10:40.271701  2120 trainer.py:136] Epoch[35/200] loss: 0.24775110930204391
I0423 21:10:41.951273  2120 trainer.py:136] Epoch[36/200] loss: 0.2501418948173523
I0423 21:10:43.632745  2120 trainer.py:136] Epoch[37/200] loss: 0.24976543188095093
I0423 21:10:45.317736  2120 trainer.py:136] Epoch[38/200] loss: 0.2482393686970075
I0423 21:10:46.987270  2120 trainer.py:136] Epoch[39/200] loss: 0.2303146854043007
I0423 21:10:48.649181  2120 trainer.py:136] Epoch[40/200] loss: 0.2334451859196027
I0423 21:10:50.320907  2120 trainer.py:136] Epoch[41/200] loss: 0.24169715493917465
I0423 21:10:52.010685  2120 trainer.py:136] Epoch[42/200] loss: 0.2280677209297816
I0423 21:10:53.672322  2120 trainer.py:136] Epoch[43/200] loss: 0.2293917308251063
I0423 21:10:55.361254  2120 trainer.py:136] Epoch[44/200] loss: 0.2444240316748619
I0423 21:10:57.043762  2120 trainer.py:136] Epoch[45/200] loss: 0.235951795677344
I0423 21:10:58.722924  2120 trainer.py:136] Epoch[46/200] loss: 0.2300142621000608
I0423 21:11:00.385932  2120 trainer.py:136] Epoch[47/200] loss: 0.2182155191898346
I0423 21:11:02.047482  2120 trainer.py:136] Epoch[48/200] loss: 0.21753766387701035
I0423 21:11:03.718045  2120 trainer.py:136] Epoch[49/200] loss: 0.22338202645381292
I0423 21:11:03.750934  2120 trainer.py:142] Test: [{'precision': 0.024472573839662424, 'recall': 0.1703995254628166, 'hit_ratio': 0.38396624472573837, 'ndcg': 0.07991435243961754}]
I0423 21:11:05.421906  2120 trainer.py:136] Epoch[50/200] loss: 0.21413542479276657
I0423 21:11:07.102488  2120 trainer.py:136] Epoch[51/200] loss: 0.21876609623432158
I0423 21:11:08.784037  2120 trainer.py:136] Epoch[52/200] loss: 0.23355786750713983
I0423 21:11:10.466961  2120 trainer.py:136] Epoch[53/200] loss: 0.21696534355481464
I0423 21:11:12.149497  2120 trainer.py:136] Epoch[54/200] loss: 0.21002086997032166
I0423 21:11:13.836005  2120 trainer.py:136] Epoch[55/200] loss: 0.20527149637540182
I0423 21:11:15.519840  2120 trainer.py:136] Epoch[56/200] loss: 0.22236306667327882
I0423 21:11:17.191237  2120 trainer.py:136] Epoch[57/200] loss: 0.1987752676010132
I0423 21:11:18.866272  2120 trainer.py:136] Epoch[58/200] loss: 0.19539596140384674
I0423 21:11:20.560158  2120 trainer.py:136] Epoch[59/200] loss: 0.20915167679389318
I0423 21:11:22.212867  2120 trainer.py:136] Epoch[60/200] loss: 0.20088814347982406
I0423 21:11:23.884315  2120 trainer.py:136] Epoch[61/200] loss: 0.19298661400874456
I0423 21:11:25.554899  2120 trainer.py:136] Epoch[62/200] loss: 0.20123064865668613
I0423 21:11:27.219297  2120 trainer.py:136] Epoch[63/200] loss: 0.18444715042908985
I0423 21:11:28.902455  2120 trainer.py:136] Epoch[64/200] loss: 0.20102581679821013
I0423 21:11:30.608809  2120 trainer.py:136] Epoch[65/200] loss: 0.2020193949341774
I0423 21:11:32.297588  2120 trainer.py:136] Epoch[66/200] loss: 0.2160601610938708
I0423 21:11:33.774520  2120 trainer.py:136] Epoch[67/200] loss: 0.18715263505776722
I0423 21:11:35.125568  2120 trainer.py:136] Epoch[68/200] loss: 0.1804271675646305
I0423 21:11:36.452689  2120 trainer.py:136] Epoch[69/200] loss: 0.194126009196043
I0423 21:11:37.785547  2120 trainer.py:136] Epoch[70/200] loss: 0.1991867631673813
I0423 21:11:39.118528  2120 trainer.py:136] Epoch[71/200] loss: 0.18482061997056007
I0423 21:11:40.448690  2120 trainer.py:136] Epoch[72/200] loss: 0.18010555629928907
I0423 21:11:41.771466  2120 trainer.py:136] Epoch[73/200] loss: 0.18913157433271408
I0423 21:11:43.105604  2120 trainer.py:136] Epoch[74/200] loss: 0.1949842393398285
I0423 21:11:44.434401  2120 trainer.py:136] Epoch[75/200] loss: 0.18479900881648065
I0423 21:11:45.764213  2120 trainer.py:136] Epoch[76/200] loss: 0.1773246725400289
I0423 21:11:47.098760  2120 trainer.py:136] Epoch[77/200] loss: 0.19147262399395307
I0423 21:11:48.446244  2120 trainer.py:136] Epoch[78/200] loss: 0.18233228648702304
I0423 21:11:49.791825  2120 trainer.py:136] Epoch[79/200] loss: 0.18227439572413762
I0423 21:11:51.155369  2120 trainer.py:136] Epoch[80/200] loss: 0.1869445413351059
I0423 21:11:52.505407  2120 trainer.py:136] Epoch[81/200] loss: 0.19759060790141422
I0423 21:11:53.861821  2120 trainer.py:136] Epoch[82/200] loss: 0.1786680057644844
I0423 21:11:55.213246  2120 trainer.py:136] Epoch[83/200] loss: 0.18248266950249672
I0423 21:11:56.572699  2120 trainer.py:136] Epoch[84/200] loss: 0.19029029409090678
I0423 21:11:57.912817  2120 trainer.py:136] Epoch[85/200] loss: 0.1885302503903707
I0423 21:11:59.260926  2120 trainer.py:136] Epoch[86/200] loss: 0.17903939386208853
I0423 21:12:00.594607  2120 trainer.py:136] Epoch[87/200] loss: 0.17021515990297
I0423 21:12:01.921310  2120 trainer.py:136] Epoch[88/200] loss: 0.17899311011036237
I0423 21:12:03.252410  2120 trainer.py:136] Epoch[89/200] loss: 0.18316059907277424
I0423 21:12:04.578164  2120 trainer.py:136] Epoch[90/200] loss: 0.1804694319764773
I0423 21:12:05.915307  2120 trainer.py:136] Epoch[91/200] loss: 0.18363434424002964
I0423 21:12:07.236525  2120 trainer.py:136] Epoch[92/200] loss: 0.17470513731241227
I0423 21:12:08.587123  2120 trainer.py:136] Epoch[93/200] loss: 0.17293131252129873
I0423 21:12:09.925245  2120 trainer.py:136] Epoch[94/200] loss: 0.1760716639459133
I0423 21:12:11.270344  2120 trainer.py:136] Epoch[95/200] loss: 0.16841746047139167
I0423 21:12:12.635951  2120 trainer.py:136] Epoch[96/200] loss: 0.1748315488298734
I0423 21:12:13.958083  2120 trainer.py:136] Epoch[97/200] loss: 0.17535418669382732
I0423 21:12:15.293015  2120 trainer.py:136] Epoch[98/200] loss: 0.1603954476614793
I0423 21:12:16.630089  2120 trainer.py:136] Epoch[99/200] loss: 0.169537203758955
I0423 21:12:16.654011  2120 trainer.py:142] Test: [{'precision': 0.02531645569620251, 'recall': 0.17356219682801957, 'hit_ratio': 0.4008438818565401, 'ndcg': 0.08184912125821886}]
I0423 21:12:17.997074  2120 trainer.py:136] Epoch[100/200] loss: 0.16525701110561689
I0423 21:12:19.322181  2120 trainer.py:136] Epoch[101/200] loss: 0.17820965771873792
I0423 21:12:20.649846  2120 trainer.py:136] Epoch[102/200] loss: 0.1782097823917866
I0423 21:12:21.984921  2120 trainer.py:136] Epoch[103/200] loss: 0.17200063665707907
I0423 21:12:23.325262  2120 trainer.py:136] Epoch[104/200] loss: 0.1752200372517109
I0423 21:12:24.651043  2120 trainer.py:136] Epoch[105/200] loss: 0.1705260157585144
I0423 21:12:25.996526  2120 trainer.py:136] Epoch[106/200] loss: 0.17462623342871667
I0423 21:12:27.369384  2120 trainer.py:136] Epoch[107/200] loss: 0.16338681181271872
I0423 21:12:28.722626  2120 trainer.py:136] Epoch[108/200] loss: 0.17317409987250965
I0423 21:12:30.065252  2120 trainer.py:136] Epoch[109/200] loss: 0.17739763607581457
I0423 21:12:31.415344  2120 trainer.py:136] Epoch[110/200] loss: 0.16151680101950963
I0423 21:12:32.782926  2120 trainer.py:136] Epoch[111/200] loss: 0.16559880475203195
I0423 21:12:34.126048  2120 trainer.py:136] Epoch[112/200] loss: 0.16464314088225365
I0423 21:12:35.473657  2120 trainer.py:136] Epoch[113/200] loss: 0.15561477815111477
I0423 21:12:36.813329  2120 trainer.py:136] Epoch[114/200] loss: 0.17118071789542835
I0423 21:12:38.175266  2120 trainer.py:136] Epoch[115/200] loss: 0.16679491524895032
I0423 21:12:39.513395  2120 trainer.py:136] Epoch[116/200] loss: 0.1595004806915919
I0423 21:12:40.841075  2120 trainer.py:136] Epoch[117/200] loss: 0.16443189879258474
I0423 21:12:42.171083  2120 trainer.py:136] Epoch[118/200] loss: 0.1570774478216966
I0423 21:12:43.498199  2120 trainer.py:136] Epoch[119/200] loss: 0.17190269952019055
I0423 21:12:44.843830  2120 trainer.py:136] Epoch[120/200] loss: 0.1632264864941438
I0423 21:12:46.171022  2120 trainer.py:136] Epoch[121/200] loss: 0.15811075468858082
I0423 21:12:47.516116  2120 trainer.py:136] Epoch[122/200] loss: 0.15963165710369745
I0423 21:12:48.839852  2120 trainer.py:136] Epoch[123/200] loss: 0.1604245938360691
I0423 21:12:50.177639  2120 trainer.py:136] Epoch[124/200] loss: 0.16819856663544971
I0423 21:12:51.543185  2120 trainer.py:136] Epoch[125/200] loss: 0.16922902117172878
I0423 21:12:52.921127  2120 trainer.py:136] Epoch[126/200] loss: 0.16895848587155343
I0423 21:12:54.264201  2120 trainer.py:136] Epoch[127/200] loss: 0.16470836798350016
I0423 21:12:55.602509  2120 trainer.py:136] Epoch[128/200] loss: 0.16087406078974406
I0423 21:12:56.937656  2120 trainer.py:136] Epoch[129/200] loss: 0.16329339717825253
I0423 21:12:58.269762  2120 trainer.py:136] Epoch[130/200] loss: 0.15783894285559655
I0423 21:12:59.579508  2120 trainer.py:136] Epoch[131/200] loss: 0.15013339097301165
I0423 21:13:00.919583  2120 trainer.py:136] Epoch[132/200] loss: 0.15552979757388433
I0423 21:13:02.244749  2120 trainer.py:136] Epoch[133/200] loss: 0.15719995299975079
I0423 21:13:03.566614  2120 trainer.py:136] Epoch[134/200] loss: 0.1639602765440941
I0423 21:13:04.883794  2120 trainer.py:136] Epoch[135/200] loss: 0.15567717750867208
I0423 21:13:06.205996  2120 trainer.py:136] Epoch[136/200] loss: 0.15256266444921493
I0423 21:13:07.568667  2120 trainer.py:136] Epoch[137/200] loss: 0.15286297077933947
I0423 21:13:08.935699  2120 trainer.py:136] Epoch[138/200] loss: 0.16040341754754384
I0423 21:13:10.300715  2120 trainer.py:136] Epoch[139/200] loss: 0.16601836333672207
I0423 21:13:11.637435  2120 trainer.py:136] Epoch[140/200] loss: 0.1528611068924268
I0423 21:13:12.982484  2120 trainer.py:136] Epoch[141/200] loss: 0.16330083360274633
I0423 21:13:14.333540  2120 trainer.py:136] Epoch[142/200] loss: 0.16106823881467183
I0423 21:13:15.675136  2120 trainer.py:136] Epoch[143/200] loss: 0.15733352253834407
I0423 21:13:17.015245  2120 trainer.py:136] Epoch[144/200] loss: 0.1616750478744507
I0423 21:13:18.341401  2120 trainer.py:136] Epoch[145/200] loss: 0.16598753308256467
I0423 21:13:19.669110  2120 trainer.py:136] Epoch[146/200] loss: 0.16315877909461657
I0423 21:13:21.007185  2120 trainer.py:136] Epoch[147/200] loss: 0.15318186258276303
I0423 21:13:22.349291  2120 trainer.py:136] Epoch[148/200] loss: 0.15207237203915913
I0423 21:13:23.677218  2120 trainer.py:136] Epoch[149/200] loss: 0.1566150079170863
I0423 21:13:23.713098  2120 trainer.py:142] Test: [{'precision': 0.024050632911392384, 'recall': 0.16169332308572812, 'hit_ratio': 0.379746835443038, 'ndcg': 0.07716088671639933}]
I0423 21:13:25.047575  2120 trainer.py:136] Epoch[150/200] loss: 0.15331776663661004
I0423 21:13:26.376700  2120 trainer.py:136] Epoch[151/200] loss: 0.15853429387013118
I0423 21:13:27.716374  2120 trainer.py:136] Epoch[152/200] loss: 0.15463407536347706
I0423 21:13:29.048057  2120 trainer.py:136] Epoch[153/200] loss: 0.16650922546784083
I0423 21:13:30.377600  2120 trainer.py:136] Epoch[154/200] loss: 0.1562957614660263
I0423 21:13:31.716325  2120 trainer.py:136] Epoch[155/200] loss: 0.14961165835460027
I0423 21:13:33.047430  2120 trainer.py:136] Epoch[156/200] loss: 0.15782803644736607
I0423 21:13:34.378540  2120 trainer.py:136] Epoch[157/200] loss: 0.14966754019260406
I0423 21:13:35.716208  2120 trainer.py:136] Epoch[158/200] loss: 0.15831607927878696
I0423 21:13:37.066252  2120 trainer.py:136] Epoch[159/200] loss: 0.1568034107486407
I0423 21:13:38.393358  2120 trainer.py:136] Epoch[160/200] loss: 0.1577930601934592
I0423 21:13:39.727022  2120 trainer.py:136] Epoch[161/200] loss: 0.15690766697128614
I0423 21:13:41.059180  2120 trainer.py:136] Epoch[162/200] loss: 0.16511064991354943
I0423 21:13:42.393310  2120 trainer.py:136] Epoch[163/200] loss: 0.16302240093549092
I0423 21:13:43.713062  2120 trainer.py:136] Epoch[164/200] loss: 0.15623990098635357
I0423 21:13:45.038200  2120 trainer.py:136] Epoch[165/200] loss: 0.15347629263997078
I0423 21:13:46.383343  2120 trainer.py:136] Epoch[166/200] loss: 0.17095713565746942
I0423 21:13:47.711136  2120 trainer.py:136] Epoch[167/200] loss: 0.1586827039718628
I0423 21:13:49.024325  2120 trainer.py:136] Epoch[168/200] loss: 0.15536589498321215
I0423 21:13:50.032491  2120 trainer.py:136] Epoch[169/200] loss: 0.15711948523918787
I0423 21:13:51.064735  2120 trainer.py:136] Epoch[170/200] loss: 0.1620976614455382
I0423 21:13:52.077911  2120 trainer.py:136] Epoch[171/200] loss: 0.16878275598088902
I0423 21:13:53.057247  2120 trainer.py:136] Epoch[172/200] loss: 0.1596364530424277
I0423 21:13:54.037574  2120 trainer.py:136] Epoch[173/200] loss: 0.1595497265458107
I0423 21:13:55.026766  2120 trainer.py:136] Epoch[174/200] loss: 0.1526973254978657
I0423 21:13:56.004055  2120 trainer.py:136] Epoch[175/200] loss: 0.16321656008561453
I0423 21:13:56.984886  2120 trainer.py:136] Epoch[176/200] loss: 0.1551521512369315
I0423 21:13:57.952668  2120 trainer.py:136] Epoch[177/200] loss: 0.15137322694063188
I0423 21:13:58.932016  2120 trainer.py:136] Epoch[178/200] loss: 0.15126955981055895
I0423 21:13:59.929249  2120 trainer.py:136] Epoch[179/200] loss: 0.15699889784057935
I0423 21:14:00.918478  2120 trainer.py:136] Epoch[180/200] loss: 0.15475307827194532
I0423 21:14:01.897781  2120 trainer.py:136] Epoch[181/200] loss: 0.1554400699834029
I0423 21:14:02.893034  2120 trainer.py:136] Epoch[182/200] loss: 0.14447468072175979
I0423 21:14:03.888308  2120 trainer.py:136] Epoch[183/200] loss: 0.1553949770828088
I0423 21:14:04.878592  2120 trainer.py:136] Epoch[184/200] loss: 0.1555382266640663
I0423 21:14:05.910509  2120 trainer.py:136] Epoch[185/200] loss: 0.15087572559714318
I0423 21:14:06.948419  2120 trainer.py:136] Epoch[186/200] loss: 0.15687166675925254
I0423 21:14:07.871893  2120 trainer.py:136] Epoch[187/200] loss: 0.1547097069521745
I0423 21:14:08.525705  2120 trainer.py:136] Epoch[188/200] loss: 0.16417334402600925
I0423 21:14:09.145323  2120 trainer.py:136] Epoch[189/200] loss: 0.16045902719100316
I0423 21:14:09.789860  2120 trainer.py:136] Epoch[190/200] loss: 0.17072623521089553
I0423 21:14:10.430716  2120 trainer.py:136] Epoch[191/200] loss: 0.16212752337257066
I0423 21:14:11.077085  2120 trainer.py:136] Epoch[192/200] loss: 0.16114746853709222
I0423 21:14:11.730474  2120 trainer.py:136] Epoch[193/200] loss: 0.15385553538799285
I0423 21:14:12.362360  2120 trainer.py:136] Epoch[194/200] loss: 0.1595131906370322
I0423 21:14:13.030755  2120 trainer.py:136] Epoch[195/200] loss: 0.15463241438070932
I0423 21:14:13.607415  2120 trainer.py:136] Epoch[196/200] loss: 0.154432721187671
I0423 21:14:14.251261  2120 trainer.py:136] Epoch[197/200] loss: 0.15413100322087606
I0423 21:14:14.861909  2120 trainer.py:136] Epoch[198/200] loss: 0.1587918922305107
I0423 21:14:15.436985  2120 trainer.py:136] Epoch[199/200] loss: 0.1498218779762586
I0423 21:14:15.450939  2120 trainer.py:142] Test: [{'precision': 0.02447257383966242, 'recall': 0.16654062065454464, 'hit_ratio': 0.3881856540084388, 'ndcg': 0.07874183086996897}]
