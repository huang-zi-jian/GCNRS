I0423 00:20:07.570650 25664 trainer.py:118] Test: [{'precision': 0.013080168776371307, 'recall': 0.07103604318794192, 'hit_ratio': 0.20675105485232068, 'ndcg': 0.03979666615782113}]
I0423 00:20:10.808782 25664 trainer.py:136] Epoch[0/200] loss: 0.6945970316727956
I0423 00:20:14.030727 25664 trainer.py:136] Epoch[1/200] loss: 0.6824256400267283
I0423 00:20:17.275163 25664 trainer.py:136] Epoch[2/200] loss: 0.6727493186791738
I0423 00:20:20.481141 25664 trainer.py:136] Epoch[3/200] loss: 0.6640520016352336
I0423 00:20:23.712322 25664 trainer.py:136] Epoch[4/200] loss: 0.65319451491038
I0423 00:20:26.964113 25664 trainer.py:136] Epoch[5/200] loss: 0.6476498961448669
I0423 00:20:30.170600 25664 trainer.py:136] Epoch[6/200] loss: 0.6267073671023051
I0423 00:20:33.370630 25664 trainer.py:136] Epoch[7/200] loss: 0.6131468812624613
I0423 00:20:36.597918 25664 trainer.py:136] Epoch[8/200] loss: 0.5987664500872294
I0423 00:20:39.810894 25664 trainer.py:136] Epoch[9/200] loss: 0.5887352446715037
I0423 00:20:43.055731 25664 trainer.py:136] Epoch[10/200] loss: 0.5797013421853383
I0423 00:20:46.311064 25664 trainer.py:136] Epoch[11/200] loss: 0.5487069467703501
I0423 00:20:49.543577 25664 trainer.py:136] Epoch[12/200] loss: 0.530617086092631
I0423 00:20:52.777466 25664 trainer.py:136] Epoch[13/200] loss: 0.5300813684860866
I0423 00:20:55.991390 25664 trainer.py:136] Epoch[14/200] loss: 0.5150864015022913
I0423 00:20:59.239421 25664 trainer.py:136] Epoch[15/200] loss: 0.49981728692849475
I0423 00:21:02.477655 25664 trainer.py:136] Epoch[16/200] loss: 0.4918583075205485
I0423 00:21:05.703371 25664 trainer.py:136] Epoch[17/200] loss: 0.47021549940109253
I0423 00:21:08.923869 25664 trainer.py:136] Epoch[18/200] loss: 0.46860285103321075
I0423 00:21:12.159845 25664 trainer.py:136] Epoch[19/200] loss: 0.46233222583929695
I0423 00:21:15.370100 25664 trainer.py:136] Epoch[20/200] loss: 0.4456143935521444
I0423 00:21:18.622890 25664 trainer.py:136] Epoch[21/200] loss: 0.44456817805767057
I0423 00:21:21.818237 25664 trainer.py:136] Epoch[22/200] loss: 0.43362632592519124
I0423 00:21:25.025894 25664 trainer.py:136] Epoch[23/200] loss: 0.3993119388818741
I0423 00:21:28.224980 25664 trainer.py:136] Epoch[24/200] loss: 0.4023277034362157
I0423 00:21:31.446860 25664 trainer.py:136] Epoch[25/200] loss: 0.408016632994016
I0423 00:21:34.682684 25664 trainer.py:136] Epoch[26/200] loss: 0.3993134597937266
I0423 00:21:37.904946 25664 trainer.py:136] Epoch[27/200] loss: 0.39209419389565786
I0423 00:21:41.279873 25664 trainer.py:136] Epoch[28/200] loss: 0.38565125068028766
I0423 00:21:44.643316 25664 trainer.py:136] Epoch[29/200] loss: 0.37600470383962
I0423 00:21:47.852292 25664 trainer.py:136] Epoch[30/200] loss: 0.3668609489997228
I0423 00:21:51.061745 25664 trainer.py:136] Epoch[31/200] loss: 0.3663518726825714
I0423 00:21:54.275279 25664 trainer.py:136] Epoch[32/200] loss: 0.35373056133588154
I0423 00:21:57.654708 25664 trainer.py:136] Epoch[33/200] loss: 0.35508850316206614
I0423 00:22:00.862644 25664 trainer.py:136] Epoch[34/200] loss: 0.3557008941968282
I0423 00:22:04.076027 25664 trainer.py:136] Epoch[35/200] loss: 0.3396417597929637
I0423 00:22:07.283520 25664 trainer.py:136] Epoch[36/200] loss: 0.3403105268875758
I0423 00:22:10.476038 25664 trainer.py:136] Epoch[37/200] loss: 0.33409819304943084
I0423 00:22:13.774698 25664 trainer.py:136] Epoch[38/200] loss: 0.32596340278784436
I0423 00:22:17.015049 25664 trainer.py:136] Epoch[39/200] loss: 0.32215101619561515
I0423 00:22:20.193638 25664 trainer.py:136] Epoch[40/200] loss: 0.33437497119108833
I0423 00:22:23.378948 25664 trainer.py:136] Epoch[41/200] loss: 0.32640351752440133
I0423 00:22:26.582452 25664 trainer.py:136] Epoch[42/200] loss: 0.3100936710834503
I0423 00:22:29.814273 25664 trainer.py:136] Epoch[43/200] loss: 0.3234108775854111
I0423 00:22:33.059948 25664 trainer.py:136] Epoch[44/200] loss: 0.3162739872932434
I0423 00:22:36.229638 25664 trainer.py:136] Epoch[45/200] loss: 0.3047603746255239
I0423 00:22:39.414994 25664 trainer.py:136] Epoch[46/200] loss: 0.3068560893336932
I0423 00:22:42.690228 25664 trainer.py:136] Epoch[47/200] loss: 0.2976255973180135
I0423 00:22:45.962569 25664 trainer.py:136] Epoch[48/200] loss: 0.30193015436331433
I0423 00:22:49.313617 25664 trainer.py:136] Epoch[49/200] loss: 0.30288249254226685
I0423 00:22:49.364446 25664 trainer.py:142] Test: [{'precision': 0.020253164556962012, 'recall': 0.14246989531799656, 'hit_ratio': 0.3206751054852321, 'ndcg': 0.06877522386304308}]
I0423 00:22:52.694745 25664 trainer.py:136] Epoch[50/200] loss: 0.28910089830557506
I0423 00:22:56.051190 25664 trainer.py:136] Epoch[51/200] loss: 0.2899810289343198
I0423 00:22:59.378309 25664 trainer.py:136] Epoch[52/200] loss: 0.28236369689305624
I0423 00:23:02.604158 25664 trainer.py:136] Epoch[53/200] loss: 0.29114506642023724
I0423 00:23:05.778585 25664 trainer.py:136] Epoch[54/200] loss: 0.28404840379953383
I0423 00:23:09.084392 25664 trainer.py:136] Epoch[55/200] loss: 0.2840462694565455
I0423 00:23:12.275520 25664 trainer.py:136] Epoch[56/200] loss: 0.2675219332178434
I0423 00:23:15.485432 25664 trainer.py:136] Epoch[57/200] loss: 0.2759167154630025
I0423 00:23:18.691465 25664 trainer.py:136] Epoch[58/200] loss: 0.2798627148071925
I0423 00:23:21.842605 25664 trainer.py:136] Epoch[59/200] loss: 0.2706297670801481
I0423 00:23:25.038563 25664 trainer.py:136] Epoch[60/200] loss: 0.2630112106601397
I0423 00:23:28.234135 25664 trainer.py:136] Epoch[61/200] loss: 0.2770189166069031
I0423 00:23:31.400558 25664 trainer.py:136] Epoch[62/200] loss: 0.2661006654302279
I0423 00:23:34.564000 25664 trainer.py:136] Epoch[63/200] loss: 0.26706240475177767
I0423 00:23:37.760529 25664 trainer.py:136] Epoch[64/200] loss: 0.2646970992287
I0423 00:23:40.926649 25664 trainer.py:136] Epoch[65/200] loss: 0.2605634659528732
I0423 00:23:44.110544 25664 trainer.py:136] Epoch[66/200] loss: 0.2564960946639379
I0423 00:23:47.314101 25664 trainer.py:136] Epoch[67/200] loss: 0.25526558309793473
I0423 00:23:50.463541 25664 trainer.py:136] Epoch[68/200] loss: 0.25933726578950883
I0423 00:23:53.646486 25664 trainer.py:136] Epoch[69/200] loss: 0.2500437165300051
I0423 00:23:56.800142 25664 trainer.py:136] Epoch[70/200] loss: 0.248558014134566
I0423 00:23:59.972190 25664 trainer.py:136] Epoch[71/200] loss: 0.25591087838013965
I0423 00:24:03.165555 25664 trainer.py:136] Epoch[72/200] loss: 0.26017074336608254
I0423 00:24:06.324678 25664 trainer.py:136] Epoch[73/200] loss: 0.2524994174639384
I0423 00:24:09.497872 25664 trainer.py:136] Epoch[74/200] loss: 0.24472022155920664
I0423 00:24:12.686944 25664 trainer.py:136] Epoch[75/200] loss: 0.2393037309249242
I0423 00:24:15.975705 25664 trainer.py:136] Epoch[76/200] loss: 0.24740262081225714
I0423 00:24:19.238066 25664 trainer.py:136] Epoch[77/200] loss: 0.2576766535639763
I0423 00:24:22.538498 25664 trainer.py:136] Epoch[78/200] loss: 0.249592454234759
I0423 00:24:25.840145 25664 trainer.py:136] Epoch[79/200] loss: 0.24408368666966757
I0423 00:24:29.040133 25664 trainer.py:136] Epoch[80/200] loss: 0.24007316182057062
I0423 00:24:32.343615 25664 trainer.py:136] Epoch[81/200] loss: 0.23831792771816254
I0423 00:24:35.534093 25664 trainer.py:136] Epoch[82/200] loss: 0.23500786672035853
I0423 00:24:38.760974 25664 trainer.py:136] Epoch[83/200] loss: 0.2427553301056226
I0423 00:24:41.936008 25664 trainer.py:136] Epoch[84/200] loss: 0.23855116665363313
I0423 00:24:45.150489 25664 trainer.py:136] Epoch[85/200] loss: 0.23730625609556835
I0423 00:24:48.428090 25664 trainer.py:136] Epoch[86/200] loss: 0.23098515669504802
I0423 00:24:51.636019 25664 trainer.py:136] Epoch[87/200] loss: 0.23454172412554422
I0423 00:24:54.828662 25664 trainer.py:136] Epoch[88/200] loss: 0.2355970467130343
I0423 00:24:58.025457 25664 trainer.py:136] Epoch[89/200] loss: 0.2291381279627482
I0423 00:25:01.232082 25664 trainer.py:136] Epoch[90/200] loss: 0.2270794043938319
I0423 00:25:04.407708 25664 trainer.py:136] Epoch[91/200] loss: 0.23445567687352498
I0423 00:25:07.668479 25664 trainer.py:136] Epoch[92/200] loss: 0.233799376587073
I0423 00:25:10.890422 25664 trainer.py:136] Epoch[93/200] loss: 0.22745164185762407
I0423 00:25:14.068478 25664 trainer.py:136] Epoch[94/200] loss: 0.2268343100945155
I0423 00:25:17.256026 25664 trainer.py:136] Epoch[95/200] loss: 0.2264829362432162
I0423 00:25:20.434099 25664 trainer.py:136] Epoch[96/200] loss: 0.22618357489506405
I0423 00:25:23.622240 25664 trainer.py:136] Epoch[97/200] loss: 0.22495917330185572
I0423 00:25:26.799309 25664 trainer.py:136] Epoch[98/200] loss: 0.22524845898151397
I0423 00:25:30.006273 25664 trainer.py:136] Epoch[99/200] loss: 0.22021462221940358
I0423 00:25:30.054112 25664 trainer.py:142] Test: [{'precision': 0.01856540084388185, 'recall': 0.1251452017274802, 'hit_ratio': 0.3037974683544304, 'ndcg': 0.06185079227757033}]
I0423 00:25:33.215823 25664 trainer.py:136] Epoch[100/200] loss: 0.2197111373146375
I0423 00:25:36.378943 25664 trainer.py:136] Epoch[101/200] loss: 0.21499989330768585
I0423 00:25:39.559062 25664 trainer.py:136] Epoch[102/200] loss: 0.2287726104259491
I0423 00:25:42.750194 25664 trainer.py:136] Epoch[103/200] loss: 0.22501491904258727
I0423 00:25:45.938829 25664 trainer.py:136] Epoch[104/200] loss: 0.22280220290025074
I0423 00:25:49.140995 25664 trainer.py:136] Epoch[105/200] loss: 0.21974965333938598
I0423 00:25:52.308627 25664 trainer.py:136] Epoch[106/200] loss: 0.22455442597468694
I0423 00:25:55.465200 25664 trainer.py:136] Epoch[107/200] loss: 0.21335774411757788
I0423 00:25:58.643290 25664 trainer.py:136] Epoch[108/200] loss: 0.22204382866621017
I0423 00:26:01.804011 25664 trainer.py:136] Epoch[109/200] loss: 0.2139979839324951
I0423 00:26:04.977882 25664 trainer.py:136] Epoch[110/200] loss: 0.21744664708773295
I0423 00:26:08.152612 25664 trainer.py:136] Epoch[111/200] loss: 0.21426005512475968
I0423 00:26:11.330192 25664 trainer.py:136] Epoch[112/200] loss: 0.22309676458438238
I0423 00:26:14.514390 25664 trainer.py:136] Epoch[113/200] loss: 0.21469491670529048
I0423 00:26:17.698431 25664 trainer.py:136] Epoch[114/200] loss: 0.21841575354337692
I0423 00:26:20.855068 25664 trainer.py:136] Epoch[115/200] loss: 0.20894766449928284
I0423 00:26:24.064016 25664 trainer.py:136] Epoch[116/200] loss: 0.2155943900346756
I0423 00:26:27.264528 25664 trainer.py:136] Epoch[117/200] loss: 0.21913413604100546
I0423 00:26:30.433934 25664 trainer.py:136] Epoch[118/200] loss: 0.2155018001794815
I0423 00:26:33.592059 25664 trainer.py:136] Epoch[119/200] loss: 0.21949787040551502
I0423 00:26:36.778136 25664 trainer.py:136] Epoch[120/200] loss: 0.21130018681287766
I0423 00:26:39.958861 25664 trainer.py:136] Epoch[121/200] loss: 0.20308048774798712
I0423 00:26:43.127552 25664 trainer.py:136] Epoch[122/200] loss: 0.20695013751586278
I0423 00:26:46.284423 25664 trainer.py:136] Epoch[123/200] loss: 0.20699912110964458
I0423 00:26:49.484048 25664 trainer.py:136] Epoch[124/200] loss: 0.2121589596072833
I0423 00:26:52.616295 25664 trainer.py:136] Epoch[125/200] loss: 0.20394887377818424
I0423 00:26:55.780802 25664 trainer.py:136] Epoch[126/200] loss: 0.20945164064566293
I0423 00:26:58.921113 25664 trainer.py:136] Epoch[127/200] loss: 0.21186506549517314
I0423 00:27:02.078813 25664 trainer.py:136] Epoch[128/200] loss: 0.20904782315095266
I0423 00:27:05.341028 25664 trainer.py:136] Epoch[129/200] loss: 0.20753171642621357
I0423 00:27:08.630276 25664 trainer.py:136] Epoch[130/200] loss: 0.20701726277669272
I0423 00:27:11.835281 25664 trainer.py:136] Epoch[131/200] loss: 0.21710867037375767
I0423 00:27:15.094069 25664 trainer.py:136] Epoch[132/200] loss: 0.21055644502242407
I0423 00:27:18.478441 25664 trainer.py:136] Epoch[133/200] loss: 0.21167100518941878
I0423 00:27:21.722832 25664 trainer.py:136] Epoch[134/200] loss: 0.2118290627996127
I0423 00:27:24.929578 25664 trainer.py:136] Epoch[135/200] loss: 0.21293505678574245
I0423 00:27:28.156438 25664 trainer.py:136] Epoch[136/200] loss: 0.21048087477684022
I0423 00:27:31.341464 25664 trainer.py:136] Epoch[137/200] loss: 0.2168450251221657
I0423 00:27:34.511537 25664 trainer.py:136] Epoch[138/200] loss: 0.20767332861820856
I0423 00:27:37.692548 25664 trainer.py:136] Epoch[139/200] loss: 0.20989609708388646
I0423 00:27:40.920417 25664 trainer.py:136] Epoch[140/200] loss: 0.2033182993531227
I0423 00:27:44.149547 25664 trainer.py:136] Epoch[141/200] loss: 0.21126118699709576
I0423 00:27:47.300690 25664 trainer.py:136] Epoch[142/200] loss: 0.2074313133955002
I0423 00:27:50.544473 25664 trainer.py:136] Epoch[143/200] loss: 0.21028528263171514
I0423 00:27:53.775057 25664 trainer.py:136] Epoch[144/200] loss: 0.2042951802412669
I0423 00:27:57.081425 25664 trainer.py:136] Epoch[145/200] loss: 0.20674604127804438
I0423 00:28:00.354585 25664 trainer.py:136] Epoch[146/200] loss: 0.21074629525343577
I0423 00:28:03.571975 25664 trainer.py:136] Epoch[147/200] loss: 0.20461322019497555
I0423 00:28:06.779946 25664 trainer.py:136] Epoch[148/200] loss: 0.20994423528512318
I0423 00:28:09.934290 25664 trainer.py:136] Epoch[149/200] loss: 0.20410472750663758
I0423 00:28:09.990103 25664 trainer.py:142] Test: [{'precision': 0.018776371308016876, 'recall': 0.13493549347979725, 'hit_ratio': 0.29957805907172996, 'ndcg': 0.06413564659735743}]
I0423 00:28:13.170715 25664 trainer.py:136] Epoch[150/200] loss: 0.20010771105686823
I0423 00:28:16.344418 25664 trainer.py:136] Epoch[151/200] loss: 0.20178238997856776
I0423 00:28:19.487825 25664 trainer.py:136] Epoch[152/200] loss: 0.2058294768134753
I0423 00:28:22.762538 25664 trainer.py:136] Epoch[153/200] loss: 0.20051497022310893
I0423 00:28:25.958530 25664 trainer.py:136] Epoch[154/200] loss: 0.20571854263544082
I0423 00:28:29.114661 25664 trainer.py:136] Epoch[155/200] loss: 0.20848251978556315
I0423 00:28:32.405029 25664 trainer.py:136] Epoch[156/200] loss: 0.19655890613794327
I0423 00:28:35.706425 25664 trainer.py:136] Epoch[157/200] loss: 0.20428686887025832
I0423 00:28:39.022468 25664 trainer.py:136] Epoch[158/200] loss: 0.20644038021564484
I0423 00:28:42.408931 25664 trainer.py:136] Epoch[159/200] loss: 0.20061625639597574
I0423 00:28:45.639583 25664 trainer.py:136] Epoch[160/200] loss: 0.20668340822060902
I0423 00:28:48.819649 25664 trainer.py:136] Epoch[161/200] loss: 0.2059936319788297
I0423 00:28:52.075537 25664 trainer.py:136] Epoch[162/200] loss: 0.2017932116985321
I0423 00:28:55.236190 25664 trainer.py:136] Epoch[163/200] loss: 0.2059116169810295
I0423 00:28:58.437174 25664 trainer.py:136] Epoch[164/200] loss: 0.20289217034975687
I0423 00:29:01.651931 25664 trainer.py:136] Epoch[165/200] loss: 0.1997575615843137
I0423 00:29:04.855942 25664 trainer.py:136] Epoch[166/200] loss: 0.20255486021439234
I0423 00:29:08.118720 25664 trainer.py:136] Epoch[167/200] loss: 0.1988706111907959
I0423 00:29:11.298949 25664 trainer.py:136] Epoch[168/200] loss: 0.2004794289668401
I0423 00:29:14.513975 25664 trainer.py:136] Epoch[169/200] loss: 0.20926614850759506
I0423 00:29:17.722985 25664 trainer.py:136] Epoch[170/200] loss: 0.20581065267324447
I0423 00:29:20.992294 25664 trainer.py:136] Epoch[171/200] loss: 0.19861216396093367
I0423 00:29:24.317287 25664 trainer.py:136] Epoch[172/200] loss: 0.20500908692677816
I0423 00:29:27.658833 25664 trainer.py:136] Epoch[173/200] loss: 0.2035689100623131
I0423 00:29:30.998343 25664 trainer.py:136] Epoch[174/200] loss: 0.19734892398118972
I0423 00:29:34.267145 25664 trainer.py:136] Epoch[175/200] loss: 0.20027150412400563
I0423 00:29:37.459154 25664 trainer.py:136] Epoch[176/200] loss: 0.20096436540285748
I0423 00:29:40.698451 25664 trainer.py:136] Epoch[177/200] loss: 0.19599230537811915
I0423 00:29:43.864593 25664 trainer.py:136] Epoch[178/200] loss: 0.19787733803192775
I0423 00:29:47.060633 25664 trainer.py:136] Epoch[179/200] loss: 0.19774238616228104
I0423 00:29:49.926956 25664 trainer.py:136] Epoch[180/200] loss: 0.19287462731202443
I0423 00:29:52.727494 25664 trainer.py:136] Epoch[181/200] loss: 0.1954836164911588
I0423 00:29:55.639571 25664 trainer.py:136] Epoch[182/200] loss: 0.20657197535037994
I0423 00:29:58.203936 25664 trainer.py:136] Epoch[183/200] loss: 0.20202731142441432
I0423 00:30:00.759043 25664 trainer.py:136] Epoch[184/200] loss: 0.1988863815863927
I0423 00:30:03.222968 25664 trainer.py:136] Epoch[185/200] loss: 0.19968543996413549
I0423 00:30:05.415741 25664 trainer.py:136] Epoch[186/200] loss: 0.19968757182359695
I0423 00:30:07.614492 25664 trainer.py:136] Epoch[187/200] loss: 0.19761387556791304
I0423 00:30:09.836787 25664 trainer.py:136] Epoch[188/200] loss: 0.1968307912349701
I0423 00:30:11.960782 25664 trainer.py:136] Epoch[189/200] loss: 0.19862205684185028
I0423 00:30:13.973911 25664 trainer.py:136] Epoch[190/200] loss: 0.20385199636220933
I0423 00:30:15.819049 25664 trainer.py:136] Epoch[191/200] loss: 0.19685711314280827
I0423 00:30:17.662993 25664 trainer.py:136] Epoch[192/200] loss: 0.20370158304770788
I0423 00:30:19.237875 25664 trainer.py:136] Epoch[193/200] loss: 0.202655824025472
I0423 00:30:20.725450 25664 trainer.py:136] Epoch[194/200] loss: 0.19156946291526158
I0423 00:30:21.853386 25664 trainer.py:136] Epoch[195/200] loss: 0.195263309776783
I0423 00:30:22.873779 25664 trainer.py:136] Epoch[196/200] loss: 0.20192545304695766
I0423 00:30:23.604985 25664 trainer.py:136] Epoch[197/200] loss: 0.20072804242372513
I0423 00:30:24.161381 25664 trainer.py:136] Epoch[198/200] loss: 0.19770396848519642
I0423 00:30:24.565030 25664 trainer.py:136] Epoch[199/200] loss: 0.19559459934631984
I0423 00:30:24.577986 25664 trainer.py:142] Test: [{'precision': 0.01856540084388185, 'recall': 0.13609080792625097, 'hit_ratio': 0.3080168776371308, 'ndcg': 0.06585953370497613}]
